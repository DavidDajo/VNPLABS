{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1289246-0bf8-467f-b83c-05ee17d26083",
   "metadata": {},
   "source": [
    "1. Intro to Data Science\n",
    "Data Science е област која комбинира математика, статистика, програмирање и \n",
    "доменско знаење со цел извлекување на вредни информации од податоците.\n",
    "Се користи за анализа, моделирање и предвидување и има широка примена \n",
    "(бизнис, медицина, финансии итн.).\n",
    "Револуцијата на податоците\n",
    "Живееме во време кога количината на податоци експоненцијално расте. Оваа \n",
    "\"податочна револуција\" е овозможена од неколку фактори:\n",
    "- Дигитализација на скоро сите аспекти на животот (социјални мрежи, сензори, \n",
    "IoT, паметни уреди)\n",
    "- Намалување на трошоците за складирање и обработка на податоци\n",
    "- Развој на моќни алгоритми и модели за анализа на огромни количини \n",
    "податоци\n",
    "Заклучок: Податоците станаа клучен ресурс за носење подобри одлуки, откривање \n",
    "на нови трендови и автоматизација на процеси.\n",
    "Четирите парадигми на науката\n",
    "Во историјата, науката е развиена преку четири главни парадигми:\n",
    "- Емпириска наука – базирана на набљудувања и експерименти\n",
    "- Теоретска наука – користи математички модели за да ги објасни природните \n",
    "феномени\n",
    "- Компјутерска наука – симулации и моделирање со помош на компјутери\n",
    "- Data driven science – користи големи количини податоци и машинско учење \n",
    "за автоматско извлекување на модели и заклучоци\n",
    "Заклучок: Data Science овозможува нов начин на истражување каде што машините \n",
    "учат од податоци, без потреба од експлицитно програмирање.\n",
    "Традиционалното програмирање vs Машинско учење\n",
    "Традиционално програмирање Машинско учење\n",
    "Човекот пишува код Машината учи од податоци\n",
    "Влез (податоци) + Програма = Излез Влез (податоци) + Излез = Програма \n",
    "(модел)\n",
    "При нови податоци потребно е рачно \n",
    "ажурирање на кодот\n",
    "Моделот автоматски се адаптира на \n",
    "нови податоци\n",
    "2\n",
    "Машинско учење и длабоко учење (Deep Learning)\n",
    "Машинско учење (ML): Алгоритми кои учат од податоци за да направат \n",
    "предвидувања.\n",
    "Длабоко учење (DL): Подгранка на ML која користи невронски мрежи со повеќе \n",
    "слоеви за обработка на сложени податоци (слики, текст, звук).\n",
    "Разлики меѓу Data Science, Business Intelligence и Machine Learning\n",
    "Business Intelligence (BI) – Анализира податоци за носење бизнис одлуки.\n",
    "Machine Learning (ML) – Автоматско учење од податоци за предвидувања.\n",
    "Data Science (DS) – Комбинира статистика, ML и анализа на податоци.\n",
    "Заклучок: Data Science е поширок концепт кој вклучува ML, BI и Advanced Analytics.\n",
    "Разлика помеѓу бази на податоци и Data Science\n",
    "Бази на податоци Data Science\n",
    "Мали до средни податоци Масивни податоци (Big Data)\n",
    "Фокус на конзистентност и безбедност Фокус на брзина и аналитика\n",
    "Структурирани податоци (SQL)\n",
    "Неструктуирани податоци (текст, \n",
    "слики, сензори итн.)\n",
    "Вештини за Data Scientist\n",
    "Статистика и математика – веројатност, хипотези и модели.\n",
    "Програмирање – Python, R, SQL, Spark.\n",
    "Обработка на податоци – Работа со Big Data, ETL процеси.\n",
    "Машинско учење – Supervised/unsupervised learning, deep learning.\n",
    "Доменско знаење – Разбирање на индустријата (медицина, финансии, маркетинг).\n",
    "Заклучок: Data Science е мешавина од технички вештини, аналитичко \n",
    "размислување и бизнис експертиза.\n",
    "3\n",
    "Предизвици во Data Science\n",
    "Најголемите потешкотии при работа со податоци:\n",
    "Проверка на претпоставките – Дали податоците навистина ја одразуваат \n",
    "реалноста?\n",
    "Валидирање на моделите – Дали моделот ќе функционира во иднина?\n",
    "Транзиција од прототип во продукција – Чистење на податоци, интеграција со \n",
    "системи.\n",
    "Доверба во резултатите – Како да убедите менаџментот дека моделот е точен?\n",
    "Заклучок: Успехот во Data Science не зависи само од математиката, туку и од \n",
    "вештината за комуникација и имплементација на решенијата.\n",
    "Data Science во пракса\n",
    "80% од времето на Data Scientist оди на подготовка на податоците (чистење, \n",
    "трансформација). \n",
    "Заклучок: Data Science во реалноста бара многу експериментирање, а не само \n",
    "програмирање.\n",
    "4\n",
    "2. Data Science Process\n",
    "Вештачката интелигенција (AI) е гранка што создава системи способни за \n",
    "извршување задачи што обично бараат човечка интелигенција, како што се \n",
    "учење, планирање и разбирање јазици.\n",
    "Примери:\n",
    "- Гласовни асистенти – Siri, Alexa, Google Assistant\n",
    "- Препознавање лица – Користено во паметни телефони и безбедносни \n",
    "системи\n",
    "- Чатботови и виртуелни асистенти – Поддршка за корисници преку AI системи \n",
    "(ChatGPT, Meta AI)\n",
    "- Автономни возила – Tesla Autopilot и самоуправувачки автомобили\n",
    "- Генеративна интелигенција – AI создава слики, текст или музика (DALL·E, \n",
    "GPT-4, DeepDream)\n",
    "Машинско учење (Machine Learning) е гранка од вештачката интелигенција \n",
    "(AI) што користи алгоритми за да научи од податоци и да прави предвидувања \n",
    "без експлицитно програмирање.\n",
    "Примери:\n",
    "- Класификација на е-пошта (spam или не)\n",
    "- Препораки на Netflix и Amazon\n",
    "Длабоко учење (Deep Learning) е подгранка на машинското учење што користи \n",
    "невронски мрежи со повеќе слоеви за обработка на сложени податоци како \n",
    "слики, текст и звук.\n",
    "5\n",
    "Примери:\n",
    "- Face recognition \n",
    "- Автоматски превод на јазици\n",
    "Клучни задачи во Data Science процесот:\n",
    "- Чистење и обработка на податоци\n",
    "- Анализа и моделирање\n",
    "- Визуелизација и презентација на резултатите\n",
    "Категории на податоци:\n",
    "- Структурирани податоци\n",
    "- Неструктурирани податоци\n",
    "- Природен јазик\n",
    "- Податоци генерирани од машина (Machine Generated)\n",
    "- Граф базирани податоци\n",
    "- Аудио, видео и слики\n",
    "- Стриминг\n",
    "Главни чекори во Data Science Process\n",
    "1. Поставување на целта – Дефинирање на проблемот\n",
    "2️. Собирање податоци – Внатрешни или надворешни извори\n",
    "Типови на извори:\n",
    "o Внатрешни: Бази, CRM системи, Excel датотеки (може да се чуваат како \n",
    "databases, data marts, data warehouses, data lakes)\n",
    "o Надворешни: Open Data (Twitter API, Google Trends, Open data (пр. \n",
    ".org/.gov) )\n",
    "o Генерирани од машини: Сензори, логови, IoT уреди\n",
    "3. Подготовка на податоци – Отстранување грешки и некомпатибилности\n",
    "Вклучува: прочистување, трансформација и комбинирање на податоците.\n",
    "2️ типа на грешки:\n",
    "o Грешки при интерпретација (пр. возраст поголема од 2️00год.)\n",
    "6\n",
    "o Неконзистентност (пр. обележување пол со Ж во една табела,а со \n",
    "“Женски“ во друга табела)\n",
    "Чести грешки: грешки при внес, непотребни празни места, невозможни \n",
    "вредности, недостасувачки вредности, outliers.\n",
    "Аномалии и отстапки (Outliers) - неочекувани вредности што отскокнуваат од \n",
    "мнозинството.\n",
    "Решенија:\n",
    "o Отстранување на очигледни грешки\n",
    "o Истражување дали отстапките имаат некаква важност\n",
    "4️. Истражување на податоците – Графички приказ, статистики\n",
    "Истражување на податоците, нивната распределба, корелација итн.\n",
    "5️. Градење модели – избирање модели и променливи кои ќе се користат во \n",
    "моделот, извршување на моделот, евалуација и споредба на модели.\n",
    "6️. Презентација и автоматизација\n",
    "Ова е итеративен процес, значи чекорите често се повторуваат!\n",
    "7\n",
    "3. Разбирање на податоците\n",
    "“A datum is a single measurement of something on a scale that is understandable to \n",
    "both the recorder and the reader. Data are multiple such measurements.”\n",
    "Заклучок: Сѐ може да е податок.\n",
    "Извори на податоци и разновидност на податоци\n",
    "Постојат податоци кои се користат за функционирање на бизнисите, следење на \n",
    "инвентар, интеракција со клиенти, финансиски трансакции итн. Од друга страна \n",
    "постојат податоци кои се јавно достапни, податоци од социјалните медиуми, слики, \n",
    "видеа и многу текстуални информации.\n",
    "“Експлозијата“ на разновидноста на податоците се случива со развојот на IoT.\n",
    "Карактеристики на податоците (3V)\n",
    "Volume – Волумен – Колку податоци има?\n",
    "Variety – Разновидност – Колку се различни типовите на податоци?\n",
    "Velocity – Брзина – Со која брзина се генерираат нови податоци?\n",
    "Начини на прибирање податоци онлајн\n",
    "API (Application Programming Interface) – користење на множество на функции \n",
    "развиено од компанија за пристап до нивните услуги. Често се плаќа за да се \n",
    "користи. Примери: Google Map API, Facebook API, Twitter API.\n",
    "RSS (Rich Site Summary) – сумаризира често ажурирана онлајн содржина во \n",
    "стандарден формат. Бесплатно за читање доколку го има на сајтот. Пример за \n",
    "користење: кај сајтови поврзани со вести, блогови.\n",
    "Web scraping – користење на софтвер, скрипти или рачно извлекување на \n",
    "податоци од она што е прикажано на страната или што се содржи во HTML – от. \n",
    "Најчесто тие податоци се ставаат во табели.\n",
    "Dark data\n",
    "Неискористени и скриени податоци кои имаат потенцијал да создадат нови \n",
    "вредности. Тоа се инфоримации кои ги прибираат организациите, ги процесираат \n",
    "8\n",
    "и чуваат додека им траат бизнис активностите, но не успеваат да ги искористат за \n",
    "други намени.\n",
    "Streaming data\n",
    "Може да е во било која форма (структурирана/неструктурирана...). Има едно \n",
    "додатно својство во однос на “обичните“ податоци – податоците се појавуваат во \n",
    "системот кога се случува некој настан, наместо да се чуваат во складиште.\n",
    "Процесите треба да се адаптираат на овие податоци. \n",
    "Примери: “What’s trending” on Twitter, live sporting or music events, stock market.\n",
    "Lambda архитектура\n",
    "Lambda архитектурата е начин на обработка на податоци што е наменет за работа \n",
    "со огромни количини на податоци, користејќи и batch (групна) и stream (во \n",
    "реално време) обработка.\n",
    " Batch обработка се користи за да се добие детална и точна слика од сите \n",
    "податоци што се собрани во некој период.\n",
    " Stream обработка се користи за да се добијат брзи резултати од \n",
    "податоците што пристигнуваат во реално време.\n",
    "Оваа архитектура се обидува да постигне рамнотежа помеѓу:\n",
    " брзина (ниска латентност) – колку брзо добиваме резултати,\n",
    " голема обработка (throughput) – колку податоци може да се обработат,\n",
    " и отпорност на грешки (fault-tolerance) – да не се изгубат податоци при \n",
    "проблеми.\n",
    "Со други зборови, Lambda архитектурата овозможува да имаш и точни резултати \n",
    "од минатото, и брзи реакции на нови податоци, така што ги комбинира \n",
    "најдобрите карактеристики од двата света.\n",
    "Дигитални Близнаци (Digital Twins)\n",
    " Дигитален близнак е дигитална копија на некој реален објект или систем.\n",
    "9\n",
    " Од гледна точка на симулација, концептот на дигитални близнаци е \n",
    "следниот чекор во развојот на моделирање, симулации и оптимизација.\n",
    "Тоа вклучува:\n",
    " Нови алгоритми и методи за симулации во реално време на различни \n",
    "физички објекти и процеси.\n",
    " Искористување на податоците собрани од дигиталните близнаци за \n",
    "подобрување на деловните и производните процеси, како и подобрување на \n",
    "секојдневниот живот.\n",
    "Пример: Ако имаш фабричка машина, можеш да направиш нејзина дигитална \n",
    "верзија што во реално време покажува како работи, предвидува дефекти и помага \n",
    "да се донесуваат подобри одлуки.\n",
    "Езеро на податоци (Data Lake)\n",
    " Data Lake е огромен складиштен систем за податоци, базиран на евтини \n",
    "технологии, кој овозможува собирање, обработка, чување и истражување \n",
    "на сурови податоци (raw data).\n",
    " Може да се справи со големи количини на податоци што пристигнуваат брзо, \n",
    "без разлика дали се структурирани (како табели) или неструктурирани (како \n",
    "слики, видеа, текстови).\n",
    " Повеќето податоци во езерото немаат веднаш препознаена вредност, но \n",
    "се чуваат за подоцна кога може да станат корисни.\n",
    " Податоците стануваат достапни веднаш штом се внесат, и се користи \n",
    "пристапот \"Schema on Read\", што значи дека структурата на податоците \n",
    "се чита и дефинира дури кога ќе се користат, не при внесување.\n",
    " За полесно управување со разнородните податоци, Data Lake треба да \n",
    "содржи метаподатоци (информации за податоците) или семантички модел, \n",
    "за да се разбере што точно се чува и како може да се користи.\n",
    "На кратко, Data Lake е големо „езеро“ каде што се собираат различни видови на \n",
    "податоци, кои подоцна може да се анализираат и искористат кога ќе бидат \n",
    "потребни.\n",
    "10\n",
    "Типови на податоци\n",
    "1. Едноставни/атомични:\n",
    "- Нумерички (integer, float)\n",
    "- Boolean (binary/ T/F)\n",
    "- Стрингови\n",
    "2️. Датум и време\n",
    "3. Листи\n",
    "4. Dictionaries (key value pairs)\n",
    "5️. Квантитативни променливи\n",
    "- дискретни (пр. број на студенти)\n",
    "- непрекинати (пр. висина)\n",
    "6️. Категориски променливи\n",
    "- номинални (не може да се подредат – нема смисла, пр. боја на очи)\n",
    "- ординални (постои подредување/градација, пр. лошо/добро/мн. добро)\n",
    "Складирање на податоците\n",
    "Табеларни податоци – податочно множество во дводимензионална табела каде \n",
    "секоја редица претставува запис, а секоја колона карактеристика/ \n",
    "мерка/променлива/атрибут.\n",
    "Број на атрибути – димензионалност. \n",
    "Структурирани податоци – секој запис е во вид на речник (jsno, xml…)\n",
    "Полуструктурирани податоци – не сите записи се претставени со ист сет на \n",
    "клучеви или некои податоци не ја користат клуч:вредност структурата.\n",
    "Чести проблеми со податоците\n",
    "Вредности кои недостасуваат, Погрешни вредности, Неуреден формат на \n",
    "податоците, Неискористливи податоци – не може да одговорат одредено \n",
    "прашање.\n",
    "11\n",
    "Истражување на податоците: Дескриптивни статистики\n",
    "Популација – целото множество на објекти/настани кои се проучуваат.\n",
    "Примерок – репрезентативно подмножество од објектите/настаните кои се \n",
    "проучуваат. Примерокот е потребен бидејќи не секогаш е возможно да се работи \n",
    "со податоците од популацијата – поради големиот број податоци.\n",
    "Пристрасност кај примероците:\n",
    "Selection bias – некои објекти или записи е поверојатно да бидат избрани\n",
    "Volunteer/nonresponse bias – објектите/записите кои не се лесно достапни не се \n",
    "претставени.\n",
    "Просек\n",
    "Претставува “типична“ вредност од примерокот или каде е центарот на\n",
    "дистрибуцијата на податоците.\n",
    "Медијана\n",
    "Исто претставува “типична“ вредност од примерокот или каде е центарот на \n",
    "дистрибуцијата на податоците.\n",
    "12\n",
    "Просекот е чувствителен на outliers.\n",
    "Мода\n",
    "Начин да се најде “најрепрезентативна“ вредност – вредност која се појавува \n",
    "најголем број пати.\n",
    "Ранг/опсег\n",
    "Начин да се измери колку се “распрснати“ вредностите во примерокот. Покажува \n",
    "колкава е разликата помеѓу најекстремните вредности.\n",
    "Варијанса\n",
    "Мери колку во просек вредностите на примерокот отстапуваат од средната \n",
    "вредност.\n",
    "Стандардна девијација \n",
    "13\n",
    "Нормални распределби, Средна вредност, Варијанса\n",
    " Средната вредност на едно множество од вредности е просекот, односно \n",
    "збирот на сите вредности поделен со нивниот број.\n",
    " Варијансата е мерка за тоа колку се „широко“ распределени вредностите\n",
    "околу средната вредност. Поточно, варијансата е просечната квадрирана \n",
    "оддалеченост на точките од средната вредност.\n",
    " Стандардната девијација е коренот од варијансата, и исто така мери колку \n",
    "вредностите отстапуваат од просекот.\n",
    " Нормалната распределба (normal distribution) е целосно дефинирана со \n",
    "нејзината средна вредност и варијанса.\n",
    "Користење на точни статистики и графови\n",
    "За континуирани променливи со нормална распределба (пример: висина, \n",
    "температура):\n",
    " Прикажи:\n",
    "Број на примероци (N), просек (mean), стандардна девијација, минимум, \n",
    "максимум\n",
    " Користи графици:\n",
    "Хистограми, точкести графици (dot plots), box плотови, scatter плотови\n",
    "За континуирани променливи со асиметрична (skewed) распределба \n",
    "(пример: плата, време на чекање):\n",
    " Прикажи:\n",
    "Број на примероци (N), медијана, долен квартил, горен квартил, минимум, \n",
    "максимум, геометриска средина\n",
    " Користи графици:\n",
    "Хистограми, точкести графици, box плотови, scatter плотови\n",
    "За категориски променливи (пример: пол, боја, тип на производ):\n",
    " Прикажи:\n",
    "Броење на фреквенции, проценти\n",
    "14\n",
    " Користи графици и табели:\n",
    "Еднодимензионални и дводимензионални табели (one-way и two-way \n",
    "tables), бар графици\n",
    "Визуелизации\n",
    "Визуелизациите ни помагаат да ги анализираме и истражиме податоците. Тие \n",
    "помагаат да:\n",
    " Се откријат скриени шаблони и трендови\n",
    " Се формулираат или тестираат хипотези\n",
    " Се комуницираат резултатите од моделирање\n",
    " Се одреди кој е следниот чекор во анализата или моделирањето\n",
    "15\n",
    "Видови на визуелизации\n",
    "Кога правиш визуелизација, прво прашај се: Што сакаш да покажеш за твоите \n",
    "податоци?\n",
    "Дистрибуција (Distribution)\n",
    "Покажува како се распределува една променлива низ можните вредности.\n",
    "Пример: Хистограм, box plot\n",
    "Однос (Relationship)\n",
    "Покажува каков е односот помеѓу две или повеќе променливи.\n",
    "Пример: Scatter plot, bubble chart\n",
    "Состав (Composition)\n",
    "Покажува како се дели податочниот сет на подгрупи или категории.\n",
    "Пример: Pie chart, stacked bar chart\n",
    "Споредба (Comparison)\n",
    "Покажува како се споредуваат вредности или трендови помеѓу различни \n",
    "променливи или групи.\n",
    "Пример: Линиски график (line chart), групиран бар график\n",
    "Дополнително објаснување на графиците:\n",
    "Histogram\n",
    "Се користи за прикажување на распределбата на една континуирана променлива. \n",
    "Ги групира вредностите во интервали (бинови) и покажува колку податоци спаѓаат \n",
    "во секој интервал. Корисно за откривање на обликот на распределбата (нормална, \n",
    "асиметрична итн.).\n",
    "Pie chart\n",
    "Се користи за прикажување на состав (композиција), односно како целината е \n",
    "поделена на делови. Секој дел од „питата“ претставува процент или удел на \n",
    "некоја категорија во вкупниот број.\n",
    "Scatter plot\n",
    "Се користи за прикажување на односот помеѓу две континуирани променливи. \n",
    "Секој податочен пар е точка на графикот. Овој график помага да се откријат \n",
    "корелации, трендови или групирања.\n",
    "Stacked area graph\n",
    "Се користи за прикажување на составот на податоци со текот на времето, каде \n",
    "16\n",
    "што вкупната сума и нејзините делови се важни. Областа е поделена по \n",
    "категории, и се гледа како секоја категорија придонесува кон вкупната сума низ \n",
    "времето.\n",
    "Multiple histograms\n",
    "Се користат за споредба на распределби на различни групи или категории. \n",
    "Прикажуваат повеќе хистограми една до друга или преклопени, за да се спореди \n",
    "како се разликуваат податоците помеѓу групите.\n",
    "Boxplot\n",
    "Се користи за сумирање на распределбата на податоци преку пет броеви: \n",
    "минимум, долен квартил, медијана, горен квартил и максимум. Исто така ги \n",
    "покажува потенцијалните отстапки (outliers). Корисен е за споредба на \n",
    "распределби помеѓу различни групи.\n",
    "Проблеми со визуелизација\n",
    "Комплексни податоци, голема димензионалност, категориски променливи.\n",
    "Пример: при голема димензионалност тешко и бескорисно е да се направи scatter \n",
    "plot, би било полесно да се направат повеќе plots од помала димензионалност.\n",
    "17\n",
    "4. Подготовка на податоците\n",
    "Прочистување, интегрирање и трансформирање податоци.\n",
    "Овој чекор е важен за добар перформанс на моделите – garbage-in equals \n",
    "garbage-out. \n",
    "Едноставен начин на предвидување: КНН – К најблиски соседи\n",
    "KНН е алгоритам кој прави предвидувања врз основа на сличност. Тој \n",
    "функционира вака:\n",
    "1. Се избира бројот на најблиски точки (K) што ќе ги разгледуваме.\n",
    "2. Се пресметува колку е блиска новата точка до сите други (на пример со \n",
    "Евклидово растојание).\n",
    "3. Се гледаат K-те најблиски точки и нивните класи.\n",
    "4. Се предвидува класата на новата точка според мнозинството (или просекот \n",
    "ако е регресија).\n",
    "** Во примерот ? е филмот за кој треба да го најдеме жанрот, за К=3 негови соседи \n",
    "се “He’s Not Really into Dudes”, “Beautiful Woman” и “California Man”. Сите 3 \n",
    "филмови се романси па и новиот филм може да се класифицира како романса.\n",
    "18\n",
    "Пример со регресија:\n",
    "К = 3\n",
    "2 +3,7 + 3,8 = 9,5 / 3 = 3,167 часа\n",
    "Прочистување на податоците\n",
    "Потребно е да се направи прочистување на податоците од грешки при внес, \n",
    "непотребни празни места, невозможни вредности, вредности кои недостасуваат и \n",
    "аутлаери.\n",
    "Често се случуваат интерпретациски грешки – пр. возраст 15️0год. и грешки во \n",
    "конзистентноста – пр. М/Ж во една табела за пол, а машко/женско во друга табела.\n",
    "Исто така треба да се внимава да се користат исти единици мерки.\n",
    "Вредности кои недостасуваат\n",
    "Не значи дека навистина недостасуваат (пр. на прашање за годишен приход на \n",
    "населението полето ќе остане празно кај децата), но треба да се справиме со нив \n",
    "бидејќи голем број од МL техниките не можат да се справат со нив.\n",
    "Missingness Indicator Variable – треба да се користи бидејќи рупата на индивидуи \n",
    "со недостасувачка вредност може систематски да се разликува од оние кај кои таа \n",
    "вредност е измерена. Ако ги третираме како да се исти, може да дојде до \n",
    "пристрасност при квантитативна анализа на релациите и до послаби резултати \n",
    "при предвидување.\n",
    "Часови учење Поени на тест\n",
    "2 28\n",
    "3,7 35\n",
    "3,8 60\n",
    "6 80\n",
    "8 100\n",
    "? 40\n",
    "19\n",
    "На пример на прашање “Дали сте користеле опоиди?“, дел од луѓето може да не \n",
    "одговорат – Дали тоа значи дека користеле опоиди? Односно дали тоа што не \n",
    "одговориле ни носи некоја дополнителна информација. Дали треба да ги \n",
    "третираме исто како оние кои не користеле?\n",
    "Всушност овој пристап креира трета група (колона)– “не одговорил“.\n",
    "Наједноставни начини за справување со вредности кои недостасуваат\n",
    "Бришење на редици (записи) или колони (атрибути) во кои има многу вредности \n",
    "кои недостасуваат.\n",
    "Пополнување на вредностите со просек или медијана (за квантитативни \n",
    "променливи) или со мода (за категориски променливи).\n",
    "Видови на вредности кои недостасуваат\n",
    "1. Missing Completely at Random (MCAR) – веројатноста некоја вредност да \n",
    "недостасува е иста за сите записи. Како да правиме дупки во податочното \n",
    "множество. Ова е “најдобар случај“ и најдобро е да се импутираат \n",
    "вредностите.\n",
    "2. Missing at Random (MAR) – веројатноста вредноста да недостасува зависи \n",
    "само од веќе достапни информации (од други променливи во податоците).\n",
    "3. Missing Not at Random (MNAR) – веројатноста вредноста да недостасува \n",
    "зависи од информации кои не се снимени, а тие информации исто така би можеле \n",
    "да ги објаснат недостасувачките вредности.\n",
    "Примери:\n",
    "MCAR (Missing Completely At Random)\n",
    "При внесување на анкета, некои листови случајно се оштетиле и се изгубиле дел \n",
    "од податоците. Недостасувањето нема врска со никаква карактеристика на \n",
    "испитаникот.\n",
    "MAR (Missing At Random)\n",
    "Постарите испитаници почесто прескокнуваат прашања за користење на \n",
    "технологија, но нивната возраст е запишана. Недостасувањето зависи од други \n",
    "познати податоци (возраст).\n",
    "20\n",
    "MNAR (Missing Not At Random)\n",
    "Луѓето со висока плата не сакаат да ја наведат во анкетата, а податокот за платата \n",
    "недостасува. Недостасувањето зависи од самата недостасувачка вредност \n",
    "(платата), која не ја знаеме.\n",
    "Методи на импутација\n",
    "1. Импутација со просек/медијана за квантитативни вредности и со мода за \n",
    "категориски вредности.\n",
    "2️. Креирање нова променлива која е индикатор на недостаток и нејзино користење \n",
    "во модел за предвидување на одговорот.\n",
    "3. Hot deck imputation – за секоја вредност која недостасува се избира случајна \n",
    "вредност од нејзината колона со која ќе се пополни.\n",
    "4️. Моделирање на импутирањето – пополнување со предвидени вредности .\n",
    "5️. Моделирање на импутирањето со неизвесност – пополнување со предвидени \n",
    "вредности + грешка .\n",
    "21\n",
    "Аутлаери\n",
    "Аутлаер е дел од примерокот кој отстапува од останатите податоци. Најлесно се \n",
    "воочуваат со box plot. \n",
    "Аутлаерот не мора секогаш да значи дека вредноста е погрешна. На пример \n",
    "доколку се следат податоци од кредитни картички – аутлаерот би значел \n",
    "отстапување т.е можеби измама и во тој случај е важен.\n",
    "Справување со аутлаери\n",
    "Отстранување, трансформација (пр. логаритам – 2,3,4,5,6,10,100 -> 0.3, 0.47, 0.6, \n",
    "0.69, 1 , 2 ), Truncation – скратување (пр. 2,3,4,5,6,10,100 -> 2,3,4,5,6,10,10).\n",
    "Векторизирање\n",
    "Процес на претворање на сурови податоци во вектор кои ги претставува.\n",
    "22\n",
    "Справување со категориски податоци\n",
    "Категориските променливи можат да земат само ограничен и обично фиксен број \n",
    "на можни вредности.\n",
    "На пример, ако податочниот сет содржи информации за корисници, честопати ќе \n",
    "има променливи како држава, пол, старосна група итн.\n",
    "Овие променливи често се складирани како текстуални вредности кои ги \n",
    "опишуваат карактеристиките на набљудувањата. На пример, полот е опишан како \n",
    "машки (M) или женски (F).\n",
    "Многу модели во машинско учење се алгебарски.\n",
    "Тоа значи дека нивниот влез мора да биде нумерички. За да ги користиме овие \n",
    "модели, категориите прво мора да се трансформираат во броеви, пред да се \n",
    "примени алгоритамот.\n",
    "Иако некои ML библиотеки автоматски ги претвораат категоријалните податоци во \n",
    "бројки (преку вградени методи), многу други не го поддржуваат тоа и бараат рачна \n",
    "обработка.\n",
    "Енкодирање на категориски променливи\n",
    "Label Encoding\n",
    "Секоја категорија добива уникатен број.\n",
    "Пример: Male = 0, Female = 1\n",
    "23\n",
    "One-Hot Encoding\n",
    "За секоја категорија се креира посебна бинарна колона.\n",
    "Пример: Gender_Male = 1, Gender_Female = 0\n",
    "Возраст Пол Пол_машко Пол_женско\n",
    "25 М 1 0\n",
    "28 Ж 0 1\n",
    "Binary Encoding\n",
    "Категориите се претвораат прво во бројки (како кај label encoding), потоа тие бројки \n",
    "во бинарен формат.\n",
    "Корисно кога има многу категории.\n",
    "Proxy Encoding\n",
    "Се користи надворешна бројчана претстава која е поврзана со категоријата.\n",
    "Пример: наместо држава како текст, се користи просечниот БДП на таа држава или \n",
    "слична метрика зависи која е целта на истражувањето.\n",
    "Векторизирање на текстуални податоци\n",
    "Наједноставниот начин за векторизирање на текст е со вектор на бројач (count \n",
    "vector), што е еквивалент на one-hot encoding, но за зборови.\n",
    "Се започнува со креирање на вокабулар – листа на уникатни зборови од целиот \n",
    "податочен сет.\n",
    "На секој збор од вокабуларот му се доделува индекс (од 0 до големината на \n",
    "вокабуларот).\n",
    "Потоа, секое реченица или пасус се претставува со листа долга колку и \n",
    "вокабуларот.\n",
    "За секоја реченица, бројот на секој индекс во листата претставува колку пати се \n",
    "појавува тој збор во дадената реченица.\n",
    "Овој метод го игнорира редоследот на зборовите во реченицата, па затоа се \n",
    "нарекува „торба со зборови“ (bag of words).\n",
    "24\n",
    "Слики\n",
    "Сликите веќае се векторизирани, бидејќи сликата не претставува ништо друго \n",
    "освен многудимензоналена низа од броеви.\n",
    "Повеќето стандардни RGB слики со три канали се складираат како листа од \n",
    "броеви, чија должина е еднаква на:\n",
    "висина × ширина × 3\n",
    "(тројката доаѓа од црвен (red), зелен (green) и син (blue) канал).\n",
    "Пример: Слика од 100x200 пиксели има 100 × 200 × 3 = 60,000 броеви, каде \n",
    "секој број претставува интензитет на боја.\n",
    "Стандардизација и нормализација на вредности на податоци\n",
    "Некои алгоритми во машинско учење бараат нормализирање на податоците, што \n",
    "значи дека секој поединечен атрибут се трансформира така што ќе биде на иста \n",
    "нумеричка скала.\n",
    "Овој чекор е многу важен кога работиме со параметри кои имаат различни мерни \n",
    "единици и размери.\n",
    "25\n",
    "На пример, некои техники (како Евклидово растојание) користат растојание меѓу \n",
    "вредности, па затоа сите параметри треба да бидат на иста скала за да може фер \n",
    "да се споредуваат.\n",
    "Две важни трансформации:\n",
    "1. Нормализација (Normalization)\n",
    "Сите нумерички променливи се трансформираат во опсег [0, 1].\n",
    "Формула:\n",
    "2. Стандардизација (Standardization)\n",
    "Податоците се трансформираат да имаат средна вредност (mean) = 0 и \n",
    "стандардна девијација = 1.\n",
    "Формула:\n",
    "Намалување на бројот на променливи (Dimensionality Reduction)\n",
    "Понекогаш имаме преголем број променливи (атрибути) и е потребно да ги \n",
    "намалиме, затоа што не додаваат нова или корисна информација за моделот.\n",
    "Прекумерниот број на променливи:\n",
    "– Го прави моделот потежок за обработка,\n",
    "– Исто така, некои алгоритми не функционираат добро кога имаат премногу влезни \n",
    "податоци.\n",
    "На пример, техниките базирани на Евклидова оддалеченост (како KNN) \n",
    "функционираат добро само до околу 10 променливи.\n",
    "Постојат специјални методи во машинското учење кои овозможуваат намалување \n",
    "на бројот на променливи, но истовремено задржуваат што е можно повеќе од \n",
    "информацијата во податоците.\n",
    "26\n",
    "Curse of Dimensionality\n",
    "Проблемот е што кога се зголемува димензионалноста (бројот на променливи), \n",
    "волуменот на просторот расте многу брзо, па достапните податоци стануваат \n",
    "разредени (sparse).\n",
    "Оваа разреденост претставува проблем за методи што бараат статистичка \n",
    "значајност, бидејќи нема доволно информации во секој дел од просторот.\n",
    "Квадратното растојание меѓу случајни точки и некоја избрана точка, со висока \n",
    "веројатност, ќе биде блиску до просечното (или медијанското) квадратно \n",
    "растојание.\n",
    "Со други зборови, разликите меѓу точките стануваат сè помалку значајни во \n",
    "повеќедимензионални простори.\n",
    "Во такви случаи, се користат други мерки на сличност, како што е косинусна \n",
    "сличност (cosine similarity), кои се подобро прилагодени за високи димензии.\n",
    "Косинусна сличност (Cosine Similarity)\n",
    " Косинусната сличност претставува косинусот на аголот помеѓу два n\u0002димензионални вектори во n-димензионален простор.\n",
    " Таа се пресметува како скаларен производ (dot product) на двата вектори, \n",
    "поделен со производот од нивните должини (или магнинуди):\n",
    "каде што:\n",
    " A⋅B е скаларниот производ (dot product),\n",
    " ∥A∥ и ∥B∥ се должините (нормите) на векторите A и B.\n",
    "Што значи вредноста:\n",
    "1 → векторите се идентични по насока (многу слични)\n",
    "0 → векторите се ортогонални (немаат сличност)\n",
    "-1 → векторите се спротивни по насока (целосна разлика)\n",
    "27\n",
    "** Дополнително: Видови растојанија\n",
    "28\n",
    "Machine Learning 1\n",
    "Што е машинско учење (Machine Learning)?\n",
    "Машинското учење е гранка на вештачката интелигенција која се занимава со \n",
    "дизајн и развој на алгоритми што им овозможуваат на компјутерите да развиваат \n",
    "однесување базирано на емпириски податоци.\n",
    "Бидејќи интелигенцијата бара знаење, потребно е компјутерите да стекнуваат \n",
    "знаење преку податоци.\n",
    "Формална дефиниција:\n",
    "„Се вели дека компјутерска програма учи од искуство E со оглед на некоја класа \n",
    "на задачи T и мерка за перформанса P, ако нејзиниот перформанс на задачи од T, \n",
    "мерен преку P, се подобрува со искуството E.“\n",
    "Човечкото учење е сè уште многу пософистицирано од дури и најнапредните \n",
    "ML алгоритми, но компјутерите имаат предност во тоа што можат побрзо и \n",
    "пообемно да меморираат, повикуваат и обработуваат податоци.\n",
    "Видови учење\n",
    "Надгледувано учење (Supervised Learning)\n",
    " Постојат лабели (labels)\n",
    " Тренинг-податоците ги вклучуваат посакуваните излези\n",
    " Цел: предвидување на иднината\n",
    " Учиме од познати минати примери за да предвидиме што ќе се случи \n",
    "понатаму\n",
    "Ненадгледувано учење (Unsupervised Learning)\n",
    " Нема лабели (labels)\n",
    " Тренинг-податоците не ги вклучуваат посакуваните излези\n",
    " Цел: да се разбере структурата на податоците\n",
    " Анализа и групирање (кластеризација), намалување на димензионалност, \n",
    "откривање скриени обрасци\n",
    " Учиме за минатото и структурата на податоците\n",
    "29\n",
    "Учење со повторување (Reinforcement Learning)\n",
    " Учење преку награди и/или казни\n",
    " Се базира на секвенца на дејства\n",
    " Цел: да се научи која постапка носи најголема корист\n",
    " Пример: Обучување на агент (како робот или играч во игра) да донесува \n",
    "одлуки врз основа на повратна информација (reward)\n",
    "Што може да се прави со ML:\n",
    "Класификација (која категорија), Регресија (колку), Кластерирање (која група), \n",
    "Аномалија (дали нешто е необично) и Учење со повторување (која акција).\n",
    "Секој ML алгоритам има 3 компоненти:\n",
    "Репрезентација, Евалуација и Оптимизација.\n",
    "Репрезентација\n",
    "Во машинското учење, начинот на кој се претставуваат податоците и моделите\n",
    "игра клучна улога во тоа како алгоритмот ќе учи. Некои од најчестите форми на \n",
    "претставување се:\n",
    "Дрва на одлука (Decision Trees)\n",
    "Јасна, разбирлива структура заснована на прашања и гранки.\n",
    "Сетови на правила / Логички програми (Sets of Rules / Logic Programs)\n",
    "Ако-тогаш (if – then) правила за донесување одлуки.\n",
    "30\n",
    "Инстанци (Instances)\n",
    "Претставување преку примери; се користи кај алгоритми како KNN.\n",
    "Графички модели (Bayes / Markov мрежи)\n",
    "Претставуваат веројатносни врски помеѓу варијабли.\n",
    "Невронски мрежи (Neural Networks)\n",
    "Инспирирани од човечкиот мозок, се користат во deep learning.\n",
    "Машини со поддржувачки вектори (Support Vector Machines - SVM)\n",
    "Геометриски пристап за класификација и регресија.\n",
    "Ансамбл модели (Model Ensembles)\n",
    "Комбинирање на повеќе модели за подобра прецизност (на пр. Random Forest, \n",
    "Boosting).\n",
    "И други...\n",
    "Евалуација\n",
    "Во машинското учење, евалуацијата се однесува на начинот на кој ја мериме \n",
    "успешноста на моделот. Се користат различни метрики во зависност од типот на \n",
    "задачата (класификација, регресија и сл.).\n",
    "Некои од најчестите мерки за евалуација се:\n",
    "Точност (Accuracy)\n",
    "Прецизност и одзив (Precision and Recall)\n",
    "Квадратична грешка (Squared Error)\n",
    "Веројатност (Likelihood)\n",
    "Постериорна веројатност (Posterior Probability)\n",
    "Цена / Корисност (Cost / Utility)\n",
    "Маргина (Margin)\n",
    "Ентропија (Entropy)\n",
    "Кулбек-Лајблер дивергенција (Kullback-Leibler Divergence, K-L divergence)\n",
    "И други...\n",
    "31\n",
    "Оптимизација\n",
    "Во машинското учење, оптимизацијата е процес на пронаоѓање на најдобрите \n",
    "вредности за параметрите на моделот со цел да се минимизира (или максимизира) \n",
    "некоја функција на грешка или целна функција.\n",
    "Видови оптимизација:\n",
    "Комбинаторна оптимизација (Combinatorial optimization)\n",
    "Конвексна оптимизација (Convex optimization)\n",
    "Оптимизација со ограничувања (Constrained optimization)\n",
    "Користење на податоци за донесување одлуки\n",
    "Традиционален пристап:\n",
    "Човек рачно ги анализира апликациите за кредит, користејќи правила и \n",
    "интуиција.\n",
    "Пристап со машинско учење:\n",
    "Машината учи оптимални одлуки директно од податоците, без потреба од \n",
    "однапред дефинирани правила.\n",
    "Овој пристап:\n",
    "o Е поточен,\n",
    "o Се подобрува со повеќе податоци,\n",
    "o Ги користи карактеристиките (features) како кредитен рејтинг, пол, \n",
    "занимање и сл.\n",
    "Пример: Логистичка регресија за одобрување на заем\n",
    " Еден од наједноставните модели за класификација.\n",
    " Ги користи влезните карактеристики (на пр. кредитен лимит, образование, \n",
    "возраст) за да предвиди веројатност за враќање на заем.\n",
    " Коефициентите во моделот се учат автоматски од тренинг податоци.\n",
    "Клучна поента: Машинското учење автоматизира и подобрува процеси на \n",
    "донесување одлуки, особено кога има многу податоци.\n",
    "32\n",
    "Линеарни vs. нелинеарни ML алгоритми\n",
    "Кога врската меѓу влезот и излезот е сложена, едноставни модели како логистичка \n",
    "регресија не се доволни. Тогаш користиме покомплексни модели.\n",
    "Предвидување на променлива\n",
    "Сакаме да предвидиме една променлива со помош на други.\n",
    "Примери: Прегледи на YouTube видео според должина, датум, претходни \n",
    "прегледи, Оценки за филм на Netflix според претходни оцени и демографија.\n",
    "Одговорна (response) променлива (Y): тоа е она што го предвидуваме – излез.\n",
    "Пр: продажба\n",
    "Објаснувачки променливи (предиктори) (X): тоа се податоци што ги користиме \n",
    "за да го предвидиме Y\n",
    "Пр: буџет за ТВ, радио, весник\n",
    "Вистински модел vs. статистички модел\n",
    " Се претпоставува дека постои непозната функција f(X) што го поврзува Y \n",
    "со X.\n",
    "Формула: Y=f(X)+ε\n",
    "каде ε е случаен шум или грешка, што не може да се објасни преку X.\n",
    " Статистички модел е приближување на f(X).\n",
    "33\n",
    "Што прави моделот?\n",
    " Целта на моделот е да пронајде врска меѓу X и Y.\n",
    " Едноставна идеја: ако имаш вредности на X и неколку Y вредности, можеш \n",
    "да пресметаш просек на Y за да предвидиш.\n",
    "Пример: Ако за некоја вредност на X имаш 3 вредности на Y: 10, 12️, 13\n",
    "тогаш предвидениот Y би бил (10+12️+13)/3 = 11.6️7\n",
    "Предвидување vs. проценка\n",
    " Inference (проценка): важна е функцијата f, сакаме да ја разбереме.\n",
    " Prediction (предвидување): не нè интересира точното f, туку точноста на \n",
    "предвидениот излез.\n",
    "Пр: Дали ќе се случи? – не е важно зошто, туку дали можеш да го погодиш.\n",
    "Пример: Може да предвидиме Y како просек на Y вредностите на К соседите. Може \n",
    "да испробаме повеќе модели со различно К и да видиме кој дава најдобар \n",
    "резултат. Гледаме кој модел дава помала грешка со пресметување на MSE или \n",
    "RMSE.\n",
    "Евалуација на грешка (Error Evaluation)\n",
    "Ги делиме податоците на тренинг сет и тест сет.\n",
    "Моделот се тренира на дел од податоците (тренинг), а се тестира на друг дел (тест) \n",
    "за да видиме колку добро предвидува непознати вредности.\n",
    "Мерење на грешка\n",
    "Користиме функција на грешка (loss function). \n",
    "Најчеста е Mean Squared Error (MSE):\n",
    "yi – вистинска вредност\n",
    " y^i – предвидена вредност\n",
    "34\n",
    "се нарекува резидуал и ја мери грешката за и-тото предвидување.\n",
    "Исто така се користи и e Root of the Mean of the Squared Errors (RMSE):\n",
    " Model fitness\n",
    "Се прашуваме: „Дали RMSE = 5️.0 е доволно добро?“\n",
    "Ако продажбите се мери во долари – да.\n",
    "Ако се мери во центи – RMSE ќе изгледа како 5️000 → но тоа не значи дека моделот \n",
    "е полош.\n",
    "Затоа, важно е да го споредиме RMSE со нешто значајно, како што е просекот.\n",
    "R² (коефициент на определеност)\n",
    "Покажува колку добро моделот ја објаснува варијабилноста на податоците.\n",
    "35\n",
    "Линеарни модели\n",
    "Наместо да пресметуваме просек како во KNN, претпоставуваме дека постои \n",
    "линеарна врска:\n",
    "Y=β1X+β0+ε \n",
    " β1: ефект на X врз Y (slope)\n",
    " β0: пресек на правата (intercept)\n",
    "Која права е најдобра?\n",
    "Одговор: Онаа која има најмала вкупна грешка.\n",
    "Се пресметуваат резидуалите:\n",
    "Како се избираат β0 и β1?\n",
    "Се користи функција на загуба (loss function) – тука:\n",
    "36\n",
    "Целта е да се пронајдат вредности на β0 и β1 што ќе ја минимизираат оваа \n",
    "грешка.\n",
    "„Brute force“ пристап\n",
    "Пробување многу можни вредности за β0 и β1, пресемтување на грешката за \n",
    "секоја, и избирање на оние вредности со најмала загуба.\n",
    "Точна формула за коефициентите\n",
    "Без да пробуваме сите можности, можеме математички да ги изведеме \n",
    "најдобрите вредности:\n",
    "Оваа линија е регресиона линија, и го претставува најдоброто линеарно \n",
    "приближување на податоците.\n",
    "Проценка на регресионите коефициенти: Градиентски спуст (Gradient Descent)\n",
    "Еден пофлексибилен метод е:\n",
    "1. Почнуваш од некоја случајна почетна точка.\n",
    "2. Определуваш во која насока да се движиш за да го намалиш загубувањето \n",
    "(лево или десно).\n",
    "3. Ја пресметуваш наклоноста (slope) на функцијата во таа точка и:\n",
    "o чекориш надесно ако наклоноста е негативна,\n",
    "o чекориш налево ако наклоноста е позитивна.\n",
    "4. Се враќаш на чекор #1.\n",
    "37\n",
    " Знаеме дека треба да се движиме во спротивна насока од дериватот\n",
    "(наклоноста), и треба да направиме чекор пропорционален на вредноста \n",
    "на дериватот.\n",
    " Правење чекор значи:\n",
    "wᵗ⁺¹ = wᵗ + α * d\n",
    "(ова е општа форма)\n",
    " Насока спротивна на дериватот и пропорционална на него значи:\n",
    "wᵗ⁺¹ = wᵗ − α * (∂L / ∂w)\n",
    "(L е функцијата за загуба, а ∂L/∂w е дериватот во однос на w)\n",
    "Превод во постандардна нотација:\n",
    "w(t+1) = w(t) − α * (∂L / ∂w)\n",
    " α (алфа) е learning rate – брзина на учење, т.е. колкав чекор правиме \n",
    "секојпат.\n",
    " w = [w₀, w₁] – овде векторот w ги претставува регресионите коефициенти (на \n",
    "пример, пресеци и наклон во линеарна регресија).\n",
    "Краток преглед:\n",
    " Gradient Descent е алгоритам за оптимизација од прв ред, што значи дека \n",
    "користи први изводи (деривати).\n",
    " Тоа е итеративен метод, се повторува додека не се достигне минимум.\n",
    " Функцијата за загуба L се намалува во насоката на негативниот дериват.\n",
    " Брзината на учење (learning rate) е контролирана од големината на α.\n",
    "Објаснување со пример:\n",
    "Замисли дека стоиш на некој рид (график на функција) и сакаш да стигнеш во \n",
    "најниската точка (минимумот).\n",
    "1. Гледаш дали наклонот на теренот е нагоре или надолу.\n",
    "2. Ако одиш надолу, се движиш во таа насока.\n",
    "3. Ако теренот е многу стрмно надолу (голем дериват), правиш поголем чекор.\n",
    "4. Ако е рамно (дериват блиску до нула), значи си блиску до минимум.\n",
    "Така, со секој чекор се приближуваш кон точката каде што е загубата најмала —\n",
    "тоа се најдобрите можни регресиони коефициенти.\n",
    "38\n",
    "Интерпретација на предикторите\n",
    "Пример:\n",
    "ŷ = 7.5 + 0.04 × TV\n",
    "Што значи 7.5️?\n",
    "Што значи 0.04️?\n",
    "Ако ја зголемиме инвестицијата во ТВ рекламирање за 1000 долари, колку би \n",
    "очекувале да порасне продажбата?\n",
    "А што ако моделот е:\n",
    "ŷ = 7.5 + 1.01 × TV?\n",
    "Во оваа равенка на регресијата, ŷ е предвидената вредност на продажбата.\n",
    "7.5️ е почетната вредност на продажбата кога инвестицијата во ТВ рекламирање е \n",
    "нула.\n",
    "0.04️ е коефициентот за променливата TV и покажува колку ќе се зголеми \n",
    "продажбата за секое зголемување на ТВ буџетот за една единица.\n",
    "Ако единицата за TV е изразена во илјадници долари, тогаш:\n",
    "Ако ја зголемиме инвестицијата за 1000 долари, продажбата ќе порасне за 0.04️ × \n",
    "1 = 0.04️ единици.\n",
    "Ако пак користиме моделот ŷ = 7.5️ + 1.01 × TV, тогаш за секои 1000 долари \n",
    "дополнително, продажбата би се зголемила за 1.01 единици.\n",
    "Заклучок:\n",
    "Коефициентите во регресијата ни покажуваат колкаво влијание има секој\n",
    "предиктор врз зависната променлива. Но дали ќе донесеме одлуки врз основа на \n",
    "тие вредности зависи од тоа колку им веруваме, што зависи од квалитетот на \n",
    "податоците, големината на примерокот и статистичката значајност на резултатите.\n",
    "Интерпретација на ε (грешката) во нашите набљудувања\n",
    "Терминот ε го интерпретираме како шум што се појавува поради случајни \n",
    "варијации во природни системи и неточности во мерењето со научни инструменти.\n",
    "Ако ја знаевме точната форма на f(x), на пример f(x) = β₀ + β₁x, и ако немаше ε, \n",
    "тогаш проценката на β₀ и β₁ би била совршено точна.\n",
    "Но, постојат три причини поради кои не можеме целосно да им веруваме на \n",
    "вредностите на β₀ и β₁:\n",
    "39\n",
    "1. ε секогаш постои\n",
    "2. не ја знаеме точната форма на f(x)\n",
    "3. имаме ограничена големина на примерок\n",
    "Интервали на доверба за проценките на предикторите:\n",
    "Поради грешката ε, секојпат кога ќе го измериме одговорот Y за иста вредност на \n",
    "X, ќе добиеме различна вредност на Y.\n",
    "На пример, ако имаме три мерења за неколку различни вредности на X, ќе добиеме \n",
    "три различни резултати на Y, иако X е ист.\n",
    "Овој концепт води до потребата од интервали на доверба, кои ни кажуваат во кој \n",
    "опсег најверојатно се наоѓа вистинската вредност на некој параметар, како β₀ или \n",
    "β₁, со дадено ниво на сигурност, на пример 95️ проценти.\n",
    "Bootstrapping за процена на грешка при семплирање\n",
    "Bootstrapping е практика каде што се проценуваат својства на некој статистички \n",
    "показател (естиматор) со тоа што тие својства се мерат преку повторено земање \n",
    "примероци од веќе набљудуваните податоци.\n",
    "На пример, можеме да ги пресметаме β₀ и β₁ повеќе пати со тоа што случајно \n",
    "земаме примероци (со враќање) од нашата постоечка база на податоци. Потоа ја \n",
    "користиме варијансата на тие повеќекратни пресметки за да ја добиеме \n",
    "приближната вистинска варијанса на β₀ и β₁.\n",
    "Ова ни овозможува да добиеме претстава за нестабилноста или сигурноста на \n",
    "нашите проценки, без потреба од нови податоци или строги теоретски \n",
    "претпоставки.\n",
    "40\n",
    "41\n",
    "Интервали на доверба за проценките на предикторите (продолжение):\n",
    " Земаме повеќе примероци (на пример 100 пати) и секојпат ја пресметуваме \n",
    "проценката за β₀ и β₁.\n",
    " Гледаме како се менуваат тие проценки од примерок до примерок.\n",
    " Варијансата (расфрланоста) на овие вредности се нарекува стандардна \n",
    "грешка на β₀ и β₁, и се бележи како SE(β₀) и SE(β₁).\n",
    " Со помош на овие стандардни грешки можеме да пресметаме интервали на \n",
    "доверба.\n",
    "Што е интервал на доверба? Тоа е опсег на вредности за кои веруваме дека со \n",
    "n% сигурност (на пример 95️%) ја содржат вистинската вредност на параметарот \n",
    "β₁ (или β₀).\n",
    "На пример: Ако интервалот на доверба за β₁ е [0.3, 0.7], тогаш со 95️% сигурност \n",
    "веруваме дека вистинската вредност на β₁ е некаде помеѓу 0.3 и 0.7.\n",
    "Цел:\n",
    "Да ја измериме несигурноста во нашите проценки и да видиме колку можеме да \n",
    "им веруваме.\n",
    "Стандардни грешки на проценките (Standard Errors)\n",
    " Стандардната грешка (SE) ни кажува колку е нестабилна една проценка, на \n",
    "пример β₀ или β₁.\n",
    " Колку е помала стандардната грешка, толку сме посигурни дека нашата \n",
    "проценка е блиску до вистинската вредност.\n",
    "Како се пресметуваат стандардни грешки?\n",
    "Метод 1: Bootstrapping\n",
    " Правиме многу примероци од податоците (со повторување).\n",
    " За секој примерок пресметуваме β₀ и β₁.\n",
    " Го гледаме распрснувањето (варијансата) на тие вредности:\n",
    "o SE(β₀) = варијанса на β₀\n",
    "o SE(β₁) = варијанса на β₁\n",
    "42\n",
    "Метод 2: Аналитички (со формули)\n",
    "Ако знаеме варијансата на шумот (ε), можеме директно да ги пресметаме SE без \n",
    "бутстреп:\n",
    "За проценетата права:\n",
    "Тогаш формулите за стандардни грешки се:\n",
    "SE(β₁) (на наклонот): SE(β₀) (на пресекот):\n",
    "Како влијае квалитетот на податоците?\n",
    " Повеќе податоци (n ↑) → SE ↓ → поточни проценки\n",
    " Податоците да се поразновидни (поголема ∑(xᵢ − x̄)²) → SE ↓\n",
    " Помал шум/грешка (ε ↓) → SE ↓\n",
    "Заклучок: Стандардните грешки ни овозможуваат да изградиме интервали на \n",
    "доверба и да процениме колку можеме да им веруваме на нашите предиктори. \n",
    "Колку се помали SE, толку е помала несигурноста во моделот.\n",
    "43\n",
    "Класификација\n",
    "До сега, методите што ги разгледавме беа насочени кон моделирање и \n",
    "предвидување на квантитативна одговорна (response) променлива (на пр. број на \n",
    "такси превози, број на изнајмени велосипеди итн). Линеарната регресија добро\n",
    "функционира во вакви ситуации. Кога одговорната (response) променлива е \n",
    "категоријална (односно, припаѓа на категории или класи), тогаш проблемот веќе \n",
    "не се нарекува регресија, туку класификација. Целта е да се обидеме да го \n",
    "класифицираме секој случај (набљудување) во одредена категорија, позната и \n",
    "како класа или кластер, која е дефинирана од променливата Y, врз основа на сет \n",
    "од предвидувачки променливи X.\n",
    "Класификацијата е различна од регресијата бидејќи наместо да предвидуваш \n",
    "бројчена вредност, ти се обидуваш да предвидиш категорија, како што е \"да\" или\n",
    "\"не\", \"позитивно\" или \"негативно\", \"тип А\" или \"тип Б\". Ова е многу корисно во \n",
    "ситуации кога треба да се донесат одлуки базирани на групи или типови, а не \n",
    "на конкретни бројки.\n",
    "Тука не може да се користи линеарна регресија бидејќи би добиле нешто вакво:\n",
    "44\n",
    "Затоа користиме логистичка регресија.\n",
    "Замисли дека сакаш да предвидиш нешто што има само два можни исхода — на \n",
    "пример, дали некој ќе плати навреме или не (да/не), дали е болен или здрав, дали \n",
    "ќе купи производ или не.\n",
    "Обичната регресија (линеарна регресија) предвидува бројки — на пример, колку \n",
    "пари ќе потрошиш, колку километри ќе возиш. Но во нашата ситуација треба да \n",
    "предвидиме нешто што не е бројка, туку избор помеѓу две категории.\n",
    "Логистичката регресија решава тоа така што наместо да предвидува директно „да“ \n",
    "или „не“, таа предвидува колку е веројатно да се случи „да“. Тоа значи дека ни \n",
    "дава број помеѓу 0 и 1, каде 0 значи „веројатноста е многу мала“ и 1 значи \n",
    "„веројатноста е многу голема“. На пример, 0.7 значи 70% шанса дека ќе се случи \n",
    "„да“.\n",
    "Потоа, ако веројатноста е поголема од некој праг (на пример 0.5️ или 5️0%), тогаш \n",
    "предвидуваме дека одговорот е „да“, а ако е помала — „не“.\n",
    "Како работи?\n",
    "Логистичката регресија користи една посебна математичка формула (логистичка \n",
    "функција) која ги зема податоците (на пр. колку личноста има пари, колку долгови) \n",
    "и ги претвора во веројатност од 0 до 1. Тоа е како „претворач“ што го зема \n",
    "резултатот од едноставна формула и го става во границите од 0 до 1.\n",
    "Таа формула не само што ја дава веројатноста, туку ни овозможува да ја сфатиме \n",
    "важноста на секој фактор (на пр. колку годишниот приход влијае врз тоа дали ќе \n",
    "плати или не).\n",
    "Правење предвидувања\n",
    "Откако ќе го решиме логистичкиот модел, добиваме вредности за \n",
    "коефициентите:\n",
    "β₀ = -10.6513 и β₁ = 0.0055\n",
    "Овие бројки се користат во формулата на логистичката регресија за да се \n",
    "пресмета веројатноста за неплаќање (default) според билансот (balance) на \n",
    "кредитната картичка.\n",
    "Формулата изгледа вака:\n",
    "p(X) = 1 / (1 + e^-(β₀ + β₁·X))\n",
    "45\n",
    "Пример 1:\n",
    "За личност со биланс од $1,000, ја вметнуваме вредноста во формулата:\n",
    "p(1000) = 1 / (1 + e^−(−10.6️5️13 + 0.005️5️ × 1000))\n",
    "= 1 / (1 + e^−(−10.6️5️13 + 5️.5️))\n",
    "= 1 / (1 + e^−(−5️.15️13))\n",
    "= 1 / (1 + e^5.1513)\n",
    "≈ 1 / (1 + 172️.79)\n",
    "≈ 0.005️7 → под 1% шанса за неплаќање\n",
    "Пример 2:\n",
    "За личност со биланс од $2,000:\n",
    "p(2️000) = 1 / (1 + e^−(−10.6️5️13 + 0.005️5️ × 2️000))\n",
    "= 1 / (1 + e^−(0.34️87))\n",
    "≈ 1 / (1 + 0.705️5️)\n",
    "≈ 0.5️86️ → 58.6% шанса за неплаќање\n",
    "Заклучок:\n",
    "Како што се зголемува билансот на кредитната картичка, расте и веројатноста дека \n",
    "личноста нема да ја исплати навреме. Логистичката регресија ни овозможува да ја \n",
    "пресметаме оваа веројатност за секој поединец, врз основа на нивните податоци.\n",
    "46\n",
    "Multiple логистичка регресија\n",
    "Сега ќе разгледаме ситуација каде што сакаме да предвидиме бинарен исход (на \n",
    "пример: „да“ или „не“, „болен“ или „здрав“) користејќи повеќе предиктори \n",
    "(влијателни фактори).\n",
    "Што значи ова?\n",
    "Во обичната логистичка регресија имавме само еден предиктор, на пример само \n",
    "билансот на кредитна картичка (balance).\n",
    "Во multiple логистичка регресија, можеме да користиме повеќе информации за \n",
    "една личност, како на пример:\n",
    " Годишен приход (income)\n",
    " Возраст (age)\n",
    " Биланс на кредитна картичка (balance)\n",
    " Број на месечни трансакции\n",
    " и сл.\n",
    "Како изгледа формулата?\n",
    "Формулата за веројатноста станува:\n",
    "p(X) = 1 / (1 + e^−(β₀ + β₁X₁ + β₂X₂ + β₃X₃ + ... + βₙXₙ))\n",
    "Тука:\n",
    " p(X) е веројатноста за „да“ (на пр. default = Yes)\n",
    " X₁, X₂, ..., Xₙ се предикторите (влезните фактори)\n",
    " β₀, β₁, ..., βₙ се тежините/коефициентите што моделот ги учи од податоците\n",
    "Пример:\n",
    "Ако сакаме да предвидиме дали некој ќе направи неплаќање (default), можеме да \n",
    "користиме:\n",
    " X₁ = биланс на кредитна картичка\n",
    " X₂ = годишен приход\n",
    "Тогаш моделот може да изгледа вака:\n",
    "log(p / (1 - p)) = β₀ + β₁ × баланс + β₂ × приход\n",
    "47\n",
    "Со тоа, моделот ја комбинира информацијата од двата фактори за да даде по\u0002точна проценка.\n",
    "Заклучок:\n",
    "Multiple логистичка регресија е логично продолжение на едноставната \n",
    "логистичка регресија. Наместо да предвидува на основа на само еден фактор, таа \n",
    "користи повеќе фактори истовремено за да даде попрецизна веројатност за \n",
    "одреден исход.\n",
    "Mерки за класификација и дијагностичко тестирање, кои се користат за да се \n",
    "процени колку добро работи еден модел\n",
    "1. Accuracy – Точност\n",
    "Колкав процент од сите предвидувања се точни.\n",
    "Accuracy = (Точно позитивни (TP) + Точно негативни (TN)) / Вкупно случаи\n",
    "Пример: Од 100 луѓе, моделот правилно предвидел 90. Accuracy = 90%.\n",
    "2. Precision – Прецизност\n",
    "Од сите случаи за кои моделот кажал \"да\", колку навистина се \"да\".\n",
    "Precision = Точно позитивни (TP) / (Точно позитивни (TP) + Лажно позитивни (FP))\n",
    "Пример: Ако моделот предвиди 10 луѓе дека нема да платат, но само 6️ од нив \n",
    "навистина не платиле → Precision = 6️ / 10 = 6️0%.\n",
    "3. Recall – Одзив\n",
    "Од сите вистински \"да\" случаи, колку успеавме да фатиме.\n",
    "Recall = Точно позитивни (ТP) / (Точно позитивни (TP) + Лажно негативни (FN))\n",
    "Пример: Имало 2️0 луѓе што не платиле, моделот фатил 15️ → Recall = 15️ / 2️0 = \n",
    "75%.\n",
    "4. Sensitivity (Чувствителност)\n",
    "Истото што и Recall. Кажува колку добро ги препознаваме позитивните случаи.\n",
    "5. Specificity (Специфичност)\n",
    "Колку добро ги препознаваме негативните случаи.\n",
    "Specificity = Точно негативни (TN) / (Точно негативни (TN) + Лажно позитивни (FP))\n",
    "48\n",
    "Пример: Од 80 луѓе што навистина ќе платат, моделот правилно препознал 70 → \n",
    "Specificity = 70 / 80 = 87.5%.\n",
    "Confusion Matrix (Матрица на конфузија)\n",
    "Тоа е табела која визуелно прикажува колку добро работи еден класификациски \n",
    "алгоритам, т.е. како ги предвидува различните класи.\n",
    "Се нарекува и матрица на грешки, бидејќи ни покажува каде моделот прави \n",
    "точни предвидувања, а каде греши.\n",
    "Пример за бинарна класификација:\n",
    "Вистински: Да (Positive) Вистински: Не (Negative)\n",
    "Предвидено: Да True Positive (TP) False Positive (FP)\n",
    "Предвидено: Не False Negative (FN) True Negative (TN)\n",
    "Confusion matrix ни овозможува да ги пресметаме:\n",
    " Accuracy\n",
    " Precision\n",
    " Recall / Sensitivity\n",
    " Specificity\n",
    " F1 Score (баланс помеѓу precision и recall)\n",
    "49\n",
    "I know you are more confused than that matrix but we gotta go on. \n",
    "ROC Plot (Receiver Operating Characteristic)\n",
    "ROC кривата е график што се користи за оценување на перформансите на еден \n",
    "класификациски модел, особено користен кај бинарна класификација.\n",
    "Оски на ROC графикот:\n",
    " Y-оска (Vertical):\n",
    "True Positive Rate (TPR) = NTP / (NTP + NFN)\n",
    "Ова е исто што и Recall или Sensitivity\n",
    "→ Покажува колкав процент од вистинските позитивни случаи сме ги \n",
    "погодиле.\n",
    " X-оска (Horizontal):\n",
    "False Positive Rate (FPR) = NFP / (NFP + NTN)\n",
    "Ова е 1 − Specificity\n",
    "→ Покажува колкав процент од негативните случаи сме ги предвиделе \n",
    "погрешно како позитивни.\n",
    "Што прикажува ROC кривата?\n",
    "ROC кривата прикажува како се менува односот помеѓу TPR и FPR додека го \n",
    "менуваме прагот (threshold) на моделот — на пр., од p > 0.9 до p > 0.1.\n",
    "50\n",
    "Идеален модел:\n",
    "TPR = 1 и FPR = 0\n",
    "Тоа значи: ги фаќа сите \n",
    "позитивни случаи, без \n",
    "ниту една грешка кај \n",
    "негативните. \n",
    "На графикот ова е \n",
    "горниот лев агол.\n",
    "Лош модел (случаен):\n",
    "Дијагонална линија (од \n",
    "(0,0) до (1,1)).\n",
    "TPR = FPR → моделот \n",
    "претпоставува на\n",
    "случајна основа.\n",
    "Добар модел:\n",
    "Кривата е над \n",
    "дијагоналата, блиску\n",
    "до горниот лев агол.\n",
    "Колку подалеку е \n",
    "кривата од \n",
    "дијагоналата нагоре и \n",
    "лево → толку подобар е \n",
    "моделот.\n",
    "AUC (Area Under Curve):\n",
    "Мерка за „површината под ROC кривата“. Се движи од 0 до 1. Треба да биде меѓу \n",
    "0.5️ и 1.\n",
    "AUC ≈ 1 → одличен модел\n",
    "AUC ≈ 0.5 → случајно претпоставување\n",
    "AUC < 0.5 → полошо од случајно\n",
    "51\n",
    "Lift Plot (Лифт график)\n",
    "Lift графикот е алатка слична на ROC, но се користи за подобро да се оцени \n",
    "корисноста на моделот во пракса, особено во ситуации кога сакаме да ги \n",
    "идентификуваме највредните корисници или случаи (на пример: кои \n",
    "најверојатно ќе купат, ќе кликнат).\n",
    "Lift ни кажува: Колку подобро се однесува моделот во споредба со случајно \n",
    "претпоставување?\n",
    "Како се пресметува Lift?\n",
    "Lift = Precision на моделот / Precision при случаен избор\n",
    "Ако моделот има Lift = 4️ → Тоа значи дека е 4️ пати подобар од случајно \n",
    "претпоставување.\n",
    "Ако Lift = 1 → Моделот нема подобра точност од случаен избор.\n",
    "Колку повисок е Lift во првите неколку проценти од податоците, толку подобро.\n",
    "Пример:\n",
    "Ако имаш 10.000 корисници и од нив 5️00 ќе направат купување (5️%), случаен \n",
    "модел би погодил 5️%.\n",
    "Но твојот модел кај првите 10% (1.000 корисници) погодува 15️0 купувачи → \n",
    "прецизност = 15️%\n",
    "Lift = 15% / 5% = 3 → 3 пати подобар од случајно.\n",
    "52\n",
    "Machine Learning 2\n",
    "Полиноминална регресија\n",
    "Што е Полиноминална Регресија?\n",
    "Полиноминалната регресија е проширување на линеарната регресија каде што се \n",
    "додаваат степени на независната променлива (x², x³, ..., x^M) за да се добие \n",
    "нелинеарна врска.\n",
    "Формула:\n",
    "Иако изгледа нелинеарно, технички е линеарен модел бидејќи е линеарен во \n",
    "параметрите (β).\n",
    "За што се користи?\n",
    "Кога податоците не можат добро да се моделираат со права линија.\n",
    "Се користи за криволинeарно \"curve fitting\" – приближување на податоци со \n",
    "криви линии.\n",
    "Примери: Раст на население со тек на време, Висина на проектил во зависност од \n",
    "времето, Комплексни трендови во продажба или температури.\n",
    "Како изгледа во практика?\n",
    " x, x², x³... се третираат како одделни предиктори (features).\n",
    " Се гради матрица на податоци со сите степени на x до степен M.\n",
    " Се користи истата техника како кај линеарна регресија за наоѓање на \n",
    "параметрите β.\n",
    "Клучен концепт: Curve Fitting\n",
    "Со зголемување на степенот M, моделот станува се пофлексибилен.\n",
    "Ockham’s razor: користи го наједноставниот модел што добро ги објаснува \n",
    "податоците.\n",
    "Пример: Ако податоците се лесно објасниви со x², нема потреба од x⁵.\n",
    "53\n",
    "Проблем: Overfitting (претерано прилагодување)\n",
    "Ако се избере премногу висок степен на полином (на пример x^10), моделот ќе го \n",
    "следи шумот во податоците и ќе има лоша генерализација.\n",
    "Симптоми: Ниска грешка на training set, Висока грешка на test set.\n",
    "Целта е да се најде идеалната сложеност на моделот.\n",
    "Решение: Model Selection и Cross-validation\n",
    "Избор на степенот M се прави преку Cross-validation:\n",
    " Поделба на податоците во тренинг и валидациски сетови\n",
    " Измери просечната грешка за различни M\n",
    " Избери M со најниска просечна грешка\n",
    "Bias-Variance Tradeoff\n",
    " Мал степен (low M) → висок bias (моделот е премногу прост).\n",
    " Голем степен (high M) → висока variance (моделот е нестабилен).\n",
    " Цел: Рамнотежа помеѓу bias и variance.\n",
    "54\n",
    "Overfitting\n",
    "Overfitting е кога моделот премногу добро ги учи податоците од тренинг сетот, \n",
    "вклучувајќи и случајни шумови или нестандардни детали кои не се генерални \n",
    "правила. Како резултат, моделот има многу мала грешка на тренинг \n",
    "податоците, но многу слабо работи на нови, невидени податоци. Тоа значи \n",
    "дека моделот не ја фаќа вистинската структура или правило, туку спецификите на \n",
    "примерите што ги научил. \n",
    "Overfitting најчесто се случува кога моделот е премногу сложен за количината \n",
    "или квалитетот на податоците, на пример, кога користиме полином со премногу \n",
    "висок степен. За да се избегне, користиме техники како cross-validation, \n",
    "ограничување на комплексноста на моделот, или regularization.\n",
    "55\n",
    "Избор на модел (Model Selection)\n",
    "Изборот на модел претставува користење на одреден метод за да се одреди колку \n",
    "сложен треба да биде моделот. Тоа може да значи, на пример, да се изберат само \n",
    "некои од променливите (predictors) или да се избере соодветен степен на \n",
    "полиномот кај полиноминална регресија.\n",
    "Главната причина зошто правиме избор на модел е за да избегнеме overfitting, \n",
    "односно моделот да не биде премногу прилагоден на тренирачките податоци.\n",
    "Overfitting може да се случи кога:\n",
    " има премногу променливи во моделот\n",
    " просторот на карактеристики (feature space) е премногу голем\n",
    " степенот на полиномот е премногу висок\n",
    "Валидација (Validation)\n",
    "Кога правиме модел, обично ги делиме податоците на два дела:\n",
    " тренинг сет (обука) – за да го научиме моделот\n",
    " валидационен сет – за да провериме колку добро моделот работи\n",
    "На пример: ако имаш 100 податоци, можеш да искористиш 80 за тренирање и 2️0 \n",
    "за валидација. Ако моделот добро предвидува на тие 2️0, веројатно ќе работи \n",
    "добро и на други нови податоци.\n",
    "ВАЛИДАЦИСКИ СЕТ ≠ ТЕСТ СЕТ\n",
    "Кога се користи Дали влијае на моделот?\n",
    "Валидациски сет За избор и подесување Да\n",
    "Тест сет За финална проверка Не\n",
    "56\n",
    "Крос валидација (Cross-validation)\n",
    "Ова е подобра верзија на валидација. Наместо да делиш само еднаш, ги делиш \n",
    "податоците повеќе пати и во делови.\n",
    "Најпозната е K-fold cross-validation:\n",
    " Ги делиш податоците на K еднакви делови (на пример 5️).\n",
    " 4️ дела ги користиш за тренирање, а 1 дел за тестирање.\n",
    " Ова се повторува 5️ пати, така што секој дел ќе биде тест барем еднаш.\n",
    " На крајот, се зема просекот од сите резултати.\n",
    "1. Low Bias, Low Variance (горе лево)\n",
    " Точност: Висока\n",
    " Прецизност: Висока\n",
    " Сите погодоци се блиску до центарот и блиску еден до друг.\n",
    " Идеален модел – добро ги учи податоците и предвидува стабилно на нови \n",
    "примери.\n",
    "57\n",
    "2. Low Bias, High Variance (горе десно)\n",
    " Точност: Во просек добра\n",
    " Прецизност: Слаба\n",
    " Погодоците се околу центарот, но растурени.\n",
    " Моделот учел добро, но е нестабилен – на различни податоци дава \n",
    "различни резултати.\n",
    "Проблем: Overfitting – премногу добро учи деталите, но не генерализира добро\n",
    "3. High Bias, Low Variance (долу лево)\n",
    " Точност: Лоша\n",
    " Прецизност: Висока\n",
    " Сите погодоци се блиску еден до друг, но далеку од центарот.\n",
    " Моделот не ја научил вистинската шема – препрост е за проблемот.\n",
    "Проблем: Underfitting – не учи доволно, затоа не функционира добро\n",
    "4. High Bias, High Variance (долу десно)\n",
    " Точност: Лоша\n",
    " Прецизност: Лоша\n",
    " Погодоците се и растурени и далеку од центарот.\n",
    " Моделот е и препрост и нестабилен.\n",
    "Најлоша ситуација – не учи добро и не е доследен\n",
    "58\n",
    "Регуларизација\n",
    "Регуларизација е техника што се користи во машинско учење и статистика за да \n",
    "се спречи overfitting на моделот.\n",
    "Кога тренираме модел, сакаме да ја минимизираме функцијата на загуба (loss \n",
    "function) - L(θ), која мери колку лошо моделот ги предвидува излезите. \n",
    "Регуларизацијата ја модифицира оваа функција така што додава дополнителен \n",
    "термин кој го казнува моделот ако има несакани својства.\n",
    "Модифицираната функција на загуба изгледа вака:\n",
    "Lreg(θ) = L(θ) + λR(θ)\n",
    "L(θ) – оригиналната функција на загуба (на пример, средна квадратна грешка)\n",
    "R(θ) – регуларизациски термин, кој дефинира што точно сакаме да казнуваме (на \n",
    "пример, големи вредности на параметрите)\n",
    "Λ – скалар што ја контролира тежината на регуларизацијата (колку важен е тој \n",
    "термин)\n",
    "Со минимизирање на Lreg(θ), не само што се обидуваме да го намалиме бројот на \n",
    "грешки, туку и да добиеме попрост модел, кој:\n",
    " не се потпира премногу на некои од влезните карактеристики\n",
    " има помали параметри (во случај на L2️ регуларизација)\n",
    " користи помал број на параметри (во случај на L1 регуларизација)\n",
    "Најчести типови на регуларизација:\n",
    "1. L2 регуларизација (Ridge): ја казнува големината на параметрите.\n",
    "2. L1 регуларизација (Lasso): тера некои параметри да станат 0 → sparse.\n",
    " \n",
    "59\n",
    "Избор на λ \n",
    "И кај Ridge и кај LASSO регресија, гледаме дека колку е поголема вредноста на \n",
    "параметарот за регуларизација λ, толку посилно казнуваме големи вредности \n",
    "на параметрите β.\n",
    "Ако λ е многу блиску до нула, добиваме обична MSE (средна квадратна грешка), \n",
    "односно Ridge и LASSO регресијата се сведуваат на обична линеарна \n",
    "регресија.\n",
    "Ако λ е доволно голема, терминот за MSE во регуларизираната функција на \n",
    "загуба станува безначаен, и регуларизацискиот термин ќе ги принуди Ridge и \n",
    "LASSO решенијата да бидат многу блиску до нула.\n",
    "За да избегнеме произволен избор на λ, треба да го избереме со крос\u0002валидација.\n",
    "Заклучок: Регуларизацијата ни помага да добиеме поуниверзален модел, кој \n",
    "подобро функционира на нови податоци, преку контрола на сложеноста на \n",
    "моделот преку дополнителен казнувачки термин во функцијата на загуба.\n",
    "Decision trees – Дрва на одлуки\n",
    "Дрво на одлука е модел во машинско учење кој се користи за донесување одлуки. \n",
    "Тоа функционира како дијаграм што поставува прашања и врз основа на \n",
    "одговорите, ве води до крајна одлука или предвидување.\n",
    "60\n",
    "Како изгледа дрво на одлука во машинско учење?\n",
    "Во машинско учење, дрвото:\n",
    " На почетокот (коренот) поставува најважното прашање (на пр. „Возраст“).\n",
    " Секоја гранка претставува одговор („да“ или „не“).\n",
    " Се дели податочниот сет на помали делови според карактеристиките \n",
    "(features).\n",
    " На крајот на дрвото (листовите), добиваме класа (на пример: \"висок ризик\" \n",
    "или \"низок ризик\").\n",
    "Дрвата на одлуки се користат за:\n",
    " Класификација: дали некој ќе купи производ, дали е болен или не итн.\n",
    " Регресија: предвидување на бројчени вредности, на пример, цена на куќа.\n",
    "Предности Недостатоци\n",
    "Лесни за разбирање и објаснување Склоност кон overfitting\n",
    "Не бараат многу претходна \n",
    "обработка на податоци\n",
    "Малите промени во податоците можат да \n",
    "го сменат дрвото\n",
    "Работат и со категоријални и со \n",
    "бројчени податоци\n",
    "Можат да бидат нестабилни\n",
    "Генерираат јасни правила за одлука Често се помалку точни од други модели \n",
    "(без енсембли)\n",
    "Брзо тренирање и предвидување Тешко се справуваат со многу сложени \n",
    "релации\n",
    "За какви податоци работи?\n",
    " Бројчени (може да се споредуваат со прагови).\n",
    " Категоријални (потребна е енкодирање или специјално справување).\n",
    "Критериуми за делење\n",
    "1. Classification Error: мера на неточност по поделбата.\n",
    "2. Gini Index: мера на „чистота“ на групите.\n",
    "3. Entropy (Information Gain): мера на неизвесност во податоците.\n",
    "61\n",
    "Услови за застанување\n",
    " Сите примери во гранката се од иста класа.\n",
    " Премалку податоци за нова поделба.\n",
    " Максимална длабочина на дрвото.\n",
    " Премала добивка од поделбата (e.g. Gini или Entropy).\n",
    "Pruning (сечење на дрвото)\n",
    " Наместо да го ограничиме растот однапред, дрвото се гради до крај, па \n",
    "потоа се сече делови што не придонесуваат за точноста.\n",
    " Се користи cost-complexity: C(T)=Error(T)+α⋅∣T∣\n",
    "Bias vs Variance\n",
    " Мали дрва → висок bias, ниска точност.\n",
    " Големи дрва → висок variance, overfitting.\n",
    " Цел: најди баланс со крос-валидација.\n",
    "Параметри во sklearn\n",
    " max_depth, min_samples_split, criterion, min_impurity_decrease, итн.\n",
    "- се користат за да се контролира растот и прецизноста на дрвото.\n",
    "62\n",
    "Machine Learning 3\n",
    "Ensebmle models\n",
    "Енсембл моделите комбинираат повеќе модели за да се добие посилен модел.\n",
    "- Bagging, Random Forest, Boosting (XGBoost)\n",
    "Проблем со дрвата на одлука\n",
    "Лесно и брзо се толкуваат и тренираат, но при покомплексни граници/функции ни \n",
    "е потребно големо дрво – тоа има висока варијанса – доведува до overfitting.\n",
    "Bagging\n",
    "Метод за градење еден модел преку тренирање и агрегирање на повеќе модели.\n",
    "Еден начин да се намали големата варијанса на резултатите од некој експеримент \n",
    "е да се повтори експериментот повеќепати и потоа да се пресмета просекот на \n",
    "добиените резултати.\n",
    "Bootstrap – генерираме повеќе примероци од тренинг податоците, користејќи \n",
    "техника на „bootstrapping“. На секој примерок тренираме целосно decision tree\n",
    "дрво.\n",
    "Агрегирање – за даден влез пресметуваме просек на излезите од сите модели за \n",
    "тој влез.\n",
    "63\n",
    "Кај класификација, се враќа класата што е најчесто предвидена од повеќето \n",
    "модели (гласање по мнозинство).\n",
    "Кај регресија, се враќа просечната вредност од сите модели.\n",
    "Овој метод се нарекува Bagging (скратено од Bootstrap Aggregating), и е \n",
    "воведен од Breiman во 1996 година.\n",
    "Предности на bagging:\n",
    "- Експресивност – со користење на целосни дрва секој модел може да се \n",
    "справи со комплексни функции и граници на одлука.\n",
    "- Ниска варијанса – правење просек на предвидувањата на сите модели ја \n",
    "намалува варијансата на финалното (крајното) предвидување, под \n",
    "претпоставка дека сме избрале доволно голем број на дрва.\n",
    "Сепак може да се случи overfitting ако дрвата се многу големи или доколку се не \n",
    "доволно длабоки може да се случи underfitting.\n",
    "Главниот недостаток на bagging е тоа што просечениот модел не е лесен за \n",
    "интерпретација – односно, не може лесно да се следи „логиката“ зад некој излез \n",
    "преку серија одлуки засновани на вредностите на предикторите.\n",
    "Out-of-Bag Error (OOB Error)\n",
    "Со енсембл методите добиваме нова метрика за проценка на предиктивната \n",
    "ефикасност на моделот, наречена out-of-bag (OOB) грешка.\n",
    "Дадено е тренинг множество и енсембл од модели, каде секој модел е трениран \n",
    "на „bootstrap“ примерок – ја пресметуваме OOB грешката на просечниот модел на \n",
    "следниот начин:\n",
    "1. За секоја точка од тренинг множеството, пресметуваме просечна предвидена\n",
    "вредност од сите модели чиј bootstrap примерок не ја содржи таа точка. \n",
    "Потоа ја пресметуваме грешката (или квадрираната грешка) на таа просечна \n",
    "предикција.\n",
    "Ова се нарекува точка-по-точка OOB грешка (point-wise out-of-bag error).\n",
    "2. Просекот од сите точка-по-точка OOB грешки за целото тренинг множество \n",
    "го дава финалниот OOB резултат.\n",
    "64\n",
    "Во пракса, дрвата во енсемблот кај bagging често се високо корелирани.\n",
    "Замисли дека во тренинг множеството има еден многу силен предиктор, наречен \n",
    "x₁, меѓу неколку умерено важни предиктори. Поради алчниот (greedy) алгоритам \n",
    "за учење, повеќето модели во енсемблот ќе го изберат токму x₁ за првите \n",
    "делења.\n",
    "Каков е резултатот? Секое дрво станува многу слично на другите, односно тие \n",
    "се идентично распределени, па и очекуваниот излез на просечниот модел е \n",
    "приближно ист како оној на секое поединечно дрво.\n",
    "Random Forest\n",
    "Random Forest е модифицирана форма на bagging која создава енсембл од \n",
    "независни decision trees.\n",
    "За да ги декорелираме дрвата, го правиме следново:\n",
    "65\n",
    "1. Секое дрво го тренираме на свој bootstrap примерок од целото тренинг \n",
    "множество (исто како кај bagging).\n",
    "2. За секое дрво, при секое делење, случајно се избира подмножество од \n",
    "Ј ′ предиктори (од сите можни предиктори).\n",
    "Од тие Ј ′ предиктори се избира најдобриот предиктор и соодветниот праг \n",
    "(threshold) за поделба.\n",
    "Подесување (tuning) на Random forest\n",
    "RF моделите имаат неколку хиперпараметри кои треба да се подесат:\n",
    "- Бројот на предиктори кои случајно се избираат при секоја поделба\n",
    "- Вкупниот број на дрва во енсемблот\n",
    "- Минималната големина на leaf nodes\n",
    "Постојат стандардни вредности за секој од хиперпараметрите на random forest, \n",
    "препорачани од искусни практичари.\n",
    "Но генерално, овие параметри треба да се подесуваат користејќи ја Out-of-Bag \n",
    "(OOB) грешката, со што тие стануваат зависни од податоците и конкретниот \n",
    "проблем.\n",
    "Пример – број на предиктори што се избираат при секое делење:\n",
    " За класификација: √𝑁ј (каде што N е вкупниот број на предиктори)\n",
    " За регресија: N / 3 \n",
    " Користејќи ја OOB грешката, тренирањето и крос валидацијата можат да се \n",
    "направат во еден процес – тренирањето се прекинува кога OOB грешката се \n",
    "стабилизира.\n",
    "66\n",
    "Кога има многу атрибути (предиктори), но само неколку од нив се важни:\n",
    "Random Forest може да не работи добро, затоа што при секое делење од дрвото, \n",
    "има мала шанса да се избере некој од важните предиктори, па многу од дрвјата ќе \n",
    "бидат слаби.\n",
    "Зголемување на бројот на дрвја (на пример од 100 на 500):\n",
    "Обично не води до overfitting (претерано учење на податоците).\n",
    "Може само да го подобри моделот или да остане ист.\n",
    "Зошто е тоа така?\n",
    "Кога имаме повеќе дрвја, се намалува случајноста (variance) и моделот станува \n",
    "постабилен, барем колку што е стабилно едно дрво само.\n",
    "Но ако ставиме премногу дрвја (илјадници):\n",
    "Дрвјата може да почнат да личат едно на друго (се корелираат)\n",
    "Тоа може повторно да ја зголеми варијансата и да не даде дополнителна \n",
    "добивка.\n",
    "Boosting\n",
    "Boosting е енсембл техника која комбинира повеќе слаби модели (обично мали \n",
    "decision trees), тренирани секвенцијално, така што секој нов модел се обидува \n",
    "да ги поправи грешките на претходните, со цел да се добие силен и точен \n",
    "предвидувачки модел.\n",
    "67\n",
    "Два главни типа на Boosting:\n",
    "1. Адаптивен Boosting (Adaptive Boosting):\n",
    "– Им дава поголема тежина на примерите кои претходниот слаб модел ги \n",
    "предвидел погрешно.\n",
    "– Се фокусира на примерите што биле тешки за претходниот модел.\n",
    "– Пример: AdaBoost\n",
    "2. Градиентски Boosting (Gradient Boosting):\n",
    "– Секој нов модел се тренира врз остатокот (residual), односно на грешките \n",
    "што ги направил досегашниот модел.\n",
    "– Овој пристап е сличен на градиентски спуст, бидејќи постепено го \n",
    "подобрува моделот.\n",
    "– Пример: XGBoost\n",
    "(Резидуали се разликата помеѓу вистинските (набљудуваните) вредности и \n",
    "предвидените вредности од моделот)\n",
    "Gradient Boosting\n",
    "Главната идеја кај boosting е дека можеме да земеме повеќе едноставни модели \n",
    "{Tₕ} и да ги комбинираме (собереме) во еден сложен модел.\n",
    "Секој модел Tₕ поединечно може да не е добар, но нивната линеарна комбинација \n",
    "може да биде многу изразена и флексибилна.\n",
    "Прашање: Кои модели да ги вклучиме во енсемблот? Кои да бидат тежините \n",
    "(коефициентите) во комбинацијата?\n",
    "Алгоритам за Gradient Boosting\n",
    "Gradient Boosting е метод за постепено градење сложен регресиски модел, преку \n",
    "додавање на едноставни модели. Секој нов модел што го додаваме ја поправa \n",
    "слабоста на претходните.\n",
    "Чекори:\n",
    "1. Тренирај прв едноставен модел T¹ врз тренинг податоците\n",
    "{x₁, y₁}, ..., {xₙ, yₙ}\n",
    "Постави T ← T¹\n",
    "Пресметај остатоци (residuals): rᵢ = yᵢ − T(xᵢ)\n",
    "2. Тренирај нов едноставен модел T² врз остатоците (r₁, ..., rₙ)\n",
    "68\n",
    "3. Ажурирај го моделот:\n",
    "T ← T + λ · T²\n",
    "(λ = learning rate = колку „тежи“ новиот модел)\n",
    "4. Пресметај нови остатоци:\n",
    "rᵢ ← rᵢ − λ · T²(xᵢ)\n",
    "5. Повтори ги чекорите 2️–4️ додека не се исполни условот за запирање\n",
    "(на пример: број на итерации или мали остатоци)\n",
    "Learning rate – λ контролира колку новиот модел влијае врз конечниот резултат. \n",
    "Мал λ значи поспоро, но посвесно учење.\n",
    "Што е стапка на учење (learning rate)?\n",
    "Стапката на учење е бројка која кажува колку големи чекори прави алгоритамот \n",
    "кога учи (кога ги менува предвидувањата на моделот).\n",
    "Зошто е важна?\n",
    "Ако чекорите се многу мали (најмалата стапка на учење), моделот многу бавно се \n",
    "учи и ќе му треба многу време (многу итерации) да стигне до добар резултат.\n",
    "Ако чекорите се многу големи (прекумерно голема стапка), моделот може да \n",
    "„прескокнува“ околу добриот резултат и никогаш да не се стабилизира.\n",
    "69\n",
    "Како да ја избереш стапката на учење?\n",
    "Ако е константна, обично пробуваш различни вредности (пример: 0.1, 0.01, 0.001) \n",
    "и гледаш која дава најдобри резултати. Ова го правиш со помош на „крос\u0002валидација“ (тестирање на моделот на различни делови од податоците).\n",
    "Понекогаш е подобро стапката на учење да се менува во текот на учењето:\n",
    "- Кога си далеку од добриот резултат (оптимум), може да правиш поголеми \n",
    "чекори (поголема стапка).\n",
    "- Кога си близу до добриот резултат, треба да правиш помали чекори (помала \n",
    "стапка) за да не го прескокнеш.\n",
    "Заклучок: Стапката на учење е брзината со која моделот учи — треба да биде \n",
    "избалансирана за да учи доволно брзо, но и стабилно.\n",
    "XGBoost\n",
    "XGBoost е многу ефикасна имплементација на Gradient Boosting со одлуки \n",
    "(Decision Trees) и има неколку интересни карактеристики:\n",
    "Регуларизација: Може да користи L1 или L2️ регуларизација за да се спречи \n",
    "overfitting на моделот.\n",
    "Обработка на податоци со празни (sparse) вредности: Вклучува алгоритам кој\n",
    "го разбира и ефективно обработува податоците со многу празни (недостасувачки) \n",
    "вредности.\n",
    "Weighted quantile sketch: Користи посебен алгоритам за да се справи со тежините \n",
    "на податоците, што е важно кога некои податоци се поважни од други.\n",
    "Пример со планинар\n",
    "Замисли дека си планинар и се спушташ по планина до најниската \n",
    "точка (оптимум).\n",
    "Ако чекориш премногу малку (мала стапка), ќе ти треба многу \n",
    "време да стигнеш долу.\n",
    "Ако чекориш премногу големи чекори (голема стапка), може да се \n",
    "лизнеш и да се вратеш нагоре, или да прескокнеш целата низина \n",
    "и да паднеш на друга страна.\n",
    "На почеток, кога си на врвот, можеш да чекориш поголеми чекори \n",
    "за побрзо да слезеш. Кога си блиску до дното, чекориш помали \n",
    "чекори за да не пропаднеш во некоја дупка.\n",
    "70\n",
    "Паралелно учење: Користи повеќе јадра на процесорот (CPU) преку блок \n",
    "структура во дизајнот, што го забрзува тренирањето.\n",
    "Cache awareness: Секој процес (thread) има свои внатрешни бафери за \n",
    "складирање на статистики, што го подобрува перформансот.\n",
    "Out-of-core computing: Може да работи со огромни сетови на податоци кои не се \n",
    "вклопуваат во RAM меморијата, користејќи дисковен простор за оптимизирано \n",
    "учење.\n",
    "Наивен Баесов класификатор\n",
    "Наивниот Баесов класификатор е многу брз и ефикасен алгоритам за \n",
    "класификација базиран на Баесовата теорема. Се нарекува „наивен“ бидејќи \n",
    "претпоставува дека сите карактеристики (features) се независни едни од други, што \n",
    "не е секогаш точно – но често работи многу добро во пракса.\n",
    "71\n",
    "Пример: Дали пораката е Spam или Not Spam?\n",
    "Порака Класа\n",
    "“Бесплатни пари” Spam\n",
    "“Состанок денес” Not Spam\n",
    "“Освоивте награда” Spam\n",
    "“Проектна презентација” Not Spam\n",
    "Нова порака: „Бесплатна награда“\n",
    "Сакаме да ја класифицираме: Spam или Not Spam?\n",
    "Фреквенции на зборови:\n",
    "Збор Во Spam Во Not Spam\n",
    "бесплатна 1 0\n",
    "награда 1 0\n",
    "Пресметка со Laplace smoothing:\n",
    " Број на зборови во Spam = 4️\n",
    " Број на зборови во Not Spam = 4️\n",
    " Вкупно различни зборови (речник) = 6️\n",
    " P(spam)=0.5, P(not_spam)=0.5\n",
    "(Често користиме Laplace smoothing – додаваме +1 на сите броеви за да \n",
    "избегнеме деление со нула)\n",
    "За Spam: За Not Spam:\n",
    "72\n",
    "Заклучок :\n",
    "Наивниот Баесов класификатор работи и со нумерички вредности.\n",
    "Кога имаме нумерички атрибути, наместо да броиме појави (како кај текст), се \n",
    "претпоставува дека тие атрибути следат некоја веројатносна распределба —\n",
    "најчесто Гаусова (нормална) распределба.\n",
    "Оваа варијанта се вика Gaussian Naive Bayes.\n",
    "73\n",
    "Artificial Neural Networks and Deep Learning\n",
    "Невронските мрежи се алгоритми во машинското учење инспирирани од начинот \n",
    "на кој функционира човечкиот мозок. Тие се составени од вештачки неврони\n",
    "организирани во слоеви (layers) – влезен, скриени и излезен слој. Нивната задача \n",
    "е да научат шеми од податоци, на пример:\n",
    " да препознаат лице на слика,\n",
    " да класифицираат е-пошта како спам или не,\n",
    " да предвидат вредност на берзата, итн.\n",
    "Пример:\n",
    "Ако на мрежата ѝ покажеш многу слики од мачки и кучиња, таа учи да ги разликува \n",
    "преку анализа на features од сликите.\n",
    "Длабоко учење е подобласт на невронските мрежи – значи, сите модели од \n",
    "длабоко учење се невронски мрежи, но не сите невронски мрежи се \"длабоки\".\n",
    "Длабоко учење се однесува на невронски мрежи со многу скриени слоеви –\n",
    "затоа се вика \"длабоко\". Овие мрежи се многу моќни и можат:\n",
    " да препознаваат сложени шеми,\n",
    " да обработуваат слики, видео и аудио,\n",
    " да преведуваат текстови, генерираат говор, итн.\n",
    "🔹 Примери на длабоко учење:\n",
    " Convolutional Neural Networks (CNNs) – за обработка на слики,\n",
    " Recurrent Neural Networks (RNNs) – за анализа на временски податоци (на \n",
    "пр. говор, текст),\n",
    " Transformers (како ChatGPT) – за природен јазик и генерација на текст.\n",
    "74\n",
    "Анатомија на невронските мрежи\n",
    "75\n",
    "Секој јазол, или неврон, е поврзан со друг и има соодветна тежина и праг.\n",
    "Невроните се основната единица на една невронска мрежа. Кога невронот ќе се \n",
    "активира, тој ја пресметува својата состојба така што ги собира сите влезни \n",
    "вредности помножени со нивните соодветни тежини. Но, невроните секогаш имаат \n",
    "уште еден дополнителен влез – пристрасноста (bias).\n",
    "Сите неврони во една мрежа се поделени во три групи:\n",
    " Влезни неврони – примаат информации од надворешниот свет\n",
    " Скриени неврони – ја обработуваат таа информација\n",
    " Излезни неврони – даваат некаков заклучок или резултат\n",
    "Активациската функција одлучува дали еден неврон треба да се активира или \n",
    "не. Тоа значи дека таа одредува дали влезот на невронот е важен за \n",
    "предвидување, користејќи едноставни математички операции.\n",
    "Улогата на активациската функција е да изведе излезна вредност врз основа на \n",
    "збир на влезни вредности што се внесуваат во еден јазол (или слој).\n",
    "Најчести активациски функции: Sigmoid, Tanh, ReLU, leaky ReLU, Generalized \n",
    "ReLU, MaxOut, Softplus, swish.\n",
    "Sigmoid – ја претвора вредноста во број меѓу 0 и 1. Добра за веројатности, но може \n",
    "да \"заспие\" (споро учење при екстремни вредности).\n",
    "Тanh – слична на Sigmoid, но вредностите се меѓу -1 и 1. Почесто се користи \n",
    "бидејќи е посиметрична.\n",
    "ReLU (Rectified Linear Unit) – враќа 0 за негативни вредности, а истата вредност \n",
    "за позитивни. Брза и многу популарна.\n",
    "Leaky ReLU – слична на ReLU, но дозволува мала негативна вредност наместо 0 \n",
    "(за да не \"умира\" невронот).\n",
    "Generalized ReLU – проширување на ReLU со прилагодливи параметри.\n",
    "MaxOut – избира најголема вредност од неколку влезови. Моќна, но потешка за \n",
    "пресметка.\n",
    "Softplus – мазна верзија на ReLU, никогаш не дава точно 0.\n",
    "Swish – комбинација од влезот и sigmoid функција, често работи подобро од ReLU \n",
    "во сложени модели (го користи и Google).\n",
    "76\n",
    "Gradient descent\n",
    "Градиентскиот спуст е начин машината да учи од грешките и постепено да се \n",
    "подобрува.\n",
    " Машината прави предвидување → гледа колку погрешила (грешка).\n",
    " Градиентскиот спуст ѝ кажува во која насока и колку да ги смени \n",
    "параметрите за следниот пат помалку да греши.\n",
    " Ова се повторува многу пати додека грешката не се намали што е можно \n",
    "повеќе.\n",
    "Цел: Да се најдат такви вредности (параметри) со кои моделот прави најточни \n",
    "можни предвидувања.\n",
    "Оската w претставува тежините (weights) – параметрите што ги менуваме во \n",
    "моделот.\n",
    "Оската J(w) е функцијата на трошок (cost function) – таа покажува колкава е \n",
    "грешката на моделот за дадена тежина w.\n",
    "Сината крива е графикот на грешката во зависност од тежината. Целта е да \n",
    "стигнеме до најниската точка на таа крива – таму е најмала грешка, т.е. \n",
    "оптимални параметри.\n",
    "Црната точка е почетната тежина – од таму почнува учењето.\n",
    "Црната стрелка (Gradient) покажува насоката на градиентот (каде расте \n",
    "грешката).\n",
    "Црните кратки стрелки надолу покажуваат насоката во која се движиме –\n",
    "спротивна од градиентот – за да ја намалиме грешката.\n",
    "Global cost minimum е најдолната точка – таму сакаме да стигнеме.\n",
    "77\n",
    "Перцептронот е наједноставниот неврон во невронска мрежа. Тој:\n",
    "1. Прима влезови (inputs) – на пример: бројки од некои податоци.\n",
    "2. Ги множи со тежини (weights) – секој влез има важност.\n",
    "3. Ги собира (сума).\n",
    "4. Додава пристрасност (bias).\n",
    "5. Применува активациска функција – одлучува дали \"ќе се активира\" или не.\n",
    "6. Враќа резултат (излез/output).\n",
    "Овој процес се нарекува:\n",
    "Forward Propagation (Пропагација нанапред)\n",
    "Ова е првиот чекор:\n",
    "Податоците влегуваат, минуваат низ мрежата и добиваме излез.\n",
    "Секој неврон пресметува резултат и го праќа на следниот слој. Така, \n",
    "информацијата се движи нанапред низ мрежата – од влезот до излезот.\n",
    "Backpropagation (Повратна пропагација)\n",
    "Ова е вториот чекор, што се случува по forward propagation:\n",
    "Се мери грешката (разликата меѓу вистински и предвиден резултат).\n",
    "Се оди наназад низ мрежата и се пресметува како да се сменат тежините за да \n",
    "биде моделот по-точен.\n",
    "Тежините се ажурираат со помош на градиентски спуст, така што мрежата учи од \n",
    "грешките.\n",
    "Loss function – ја мери разликата т.е грешката меќу предвидувањата од моделот \n",
    "и вистинските вредности.\n",
    "78\n",
    "Невронските мрежи работат подобро и побрзо ако се нормализираат \n",
    "податоците (средна вредност 0 и стд.девијација 1)!\n",
    "Batch Normalization е техника која се користи во невронски мрежи за да се забрза \n",
    "и стабилизира учењето.\n",
    "Ги нормализира влезовите на секој слој, односно ги доведува до сличен опсег (со \n",
    "слична средна вредност и варијанса). Се применува за секој мини-беч (мал дел од \n",
    "податоците што се користи за една итерација во тренингот).\n",
    "Regularization (Регуларизација)\n",
    "Регуларизацијата е техника што ја користиме во процесот на учење на моделите \n",
    "(за да го направиме моделот подобро да се прилагодува на нови, невидени \n",
    "податоци — односно, да има помала грешка при генерализација.\n",
    "Кога моделот е премногу сложен, тој може да научи „премногу добро“ т.е да се \n",
    "прилагоди на податоците од тренинг сетот, вклучувајќи и шум или грешки во нив. \n",
    "Ова се нарекува overfitting (прекумерно прилагодување). Регуларизацијата \n",
    "помага да се спречи ова прекумерно прилагодување.\n",
    "Регуларизацијата додава некоја промена или ограничување на алгоритамот, \n",
    "која го прави моделот да биде „поизмерен“ и да не се фокусира прецизно на сите \n",
    "детали од тренинг податоците. На пример, тоа може да значи дека ќе се \n",
    "„наградува“ да има помали тежини во мрежата, или да се казнува премногу сложен \n",
    "модел.\n",
    "Заклучок: Регуларизацијата ја намалува грешката при тестирање или при внес на \n",
    "нови податоци, спречува overfitting и ја регулира сложеноста на моделот.\n",
    "Видови регуларизација:\n",
    "L1 регуларизација: Прави некои тежини да се точно нула, па моделот користи \n",
    "помалку карактеристики.\n",
    "L2 регуларизација: Ги прави тежините помали, но не ги брише, за да се избегне \n",
    "претренирање.\n",
    "Dropout: Случајно „гаси“ дел од невроните за време на тренинг, па мрежата учи \n",
    "поотпорно.\n",
    "Early Stopping: Се запира тренингот порано, кога моделот почнува да се \n",
    "влошува на нови податоци, за да не се претренира.\n",
    "79\n",
    "Advanced neural networks\n",
    "Главни видови на невронски мрежи:\n",
    "1. Feedforward Neural Networks (FFNN)\n",
    "o Наједноставни, сигналот оди само од влез кон излез (без враќање \n",
    "назад).\n",
    "o Се користат за класични задачи како класификација и регресија.\n",
    "2. Convolutional Neural Networks (CNN)\n",
    "o Специјализирани за обработка на слики и видеа.\n",
    "o Користат филтри (конволуции) за да извлечат важни карактеристики.\n",
    "3. Recurrent Neural Networks (RNN)\n",
    "o Можат да обработуваат секвенции и временски серии.\n",
    "o Имаат меморија и ја паметат претходната информација.\n",
    "4. Long Short-Term Memory (LSTM)\n",
    "o Вид на RNN, но подобро ја чува и контролира долгорочната меморија.\n",
    "o Често се користи за јазик, превод, и аудио.\n",
    "5. Generative Adversarial Networks (GANs)\n",
    "o Се состојат од две мрежи што се „борaт“ — една создава (генерира) \n",
    "нови податоци, а другата проверува дали се вистински.\n",
    "o Се користат за генерирање слики, видеа и други податоци.\n",
    "6. Autoencoders\n",
    "o Мрежи кои учат да ја реконструираат влезната информација.\n",
    "o Се користат за намалување на димензионалност и откривање \n",
    "аномалии.\n",
    "80\n",
    "Во целосно поврзаниот слој (fully \n",
    "connected layer), секој неврон е \n",
    "поврзан со сите неврони од \n",
    "претходниот слој.\n",
    "Во конволуцискиот слој (convolutional \n",
    "layer), секој неврон е поврзан само со \n",
    "ограничен број неврони во локален \n",
    "дел од претходниот слој.\n",
    "Дополнително, во конволуцискиот \n",
    "слој, сите неврони ги користат истите \n",
    "тежини за овие врски, што е \n",
    "прикажано со истиот стил на линиите.\n",
    "CNN\n",
    "Инпутот може да има голема димензија (бр. на атрибути), па користењето на \n",
    "целосно поврзана НМ би користела голем број на параметри.\n",
    "Инспирирано од експериментите на Hubel и Wiesel (1962), CNN се специјален тип \n",
    "на НМ каде што скриените слоеви се поврзани само со рецептивното поле. Бројот \n",
    "на параметри кај CNN е многу помал.\n",
    "Рецептивното поле е мал дел од влезните податоци (на пример, дел од сликата)\n",
    "на кој е „насочен“ еден неврон во конволуциска мрежа.\n",
    "Наместо невронот да гледа во целата слика одеднаш, тој гледа само мала локална \n",
    "област (на пример, 5️x5️ пиксели).\n",
    "Тоа поле му овозможува на невронот да „извлече“ локални карактеристики (како \n",
    "линии, рабови или текстури) од таа мала област.\n",
    "Потоа, повеќе неврони со поместени рецептивни полиња заедно го покриваат \n",
    "целото поле на сликата.\n",
    "81\n",
    "Три фази на конволуциски слој\n",
    "1. Convolution stage (Конволуција)\n",
    "Се применува филтер (kernel) на влезните податоци, кој „се пролизгува“ преку \n",
    "сликата или претходниот слој и извлекува локални карактеристики (на пример, \n",
    "рабови, текстури).\n",
    "Резултатот е нова „карактеристична мапа“ (feature map).\n",
    "2. Nonlinearity (Нелинеарност)\n",
    "На излезот од конволуцијата се применува нелинеарна функција, како што е ReLU\n",
    "(Rectified Linear Unit) или tanh.\n",
    "Ова ја прави мрежата способна да учи сложени и нелинеарни модели, затоа што \n",
    "без ова, конволуциите би биле само линеарни операции.\n",
    "3. Pooling (Пулинг)\n",
    "Ова е чекор за намалување на големината на излезот и за собирање важни \n",
    "информации. Најчести се:\n",
    "Max pooling – го избира најголемиот број од мала област (на пример 2️x2️ пиксели).\n",
    "Average pooling – пресметува просек од мала област.\n",
    "Со пулинг се добива резиме на локалните информации и се намалува бројот на \n",
    "параметри, што ја прави мрежата поефикасна.\n",
    "Пример 1: CNN за слика\n",
    "Класификација на сликa (пр: препознавање дали е куче или маче)\n",
    "Влез: Слика со големина 100x100 пиксели и 3 бои (RGB)\n",
    "Како работи:\n",
    "Конволуциски слој користи филтри (на пример, 5️x5️) кои „ги гледаат“ малите делови \n",
    "од сликата и извлекуваат карактеристики како рабови, бои, текстури.\n",
    "Повторени слоеви од конволуција и пулинг ја намалуваат големината, но ја \n",
    "зголемуваат комплексноста на карактеристиките.\n",
    "На крајот, целосно поврзан слој ги класифицира сликите во категории (куче, маче).\n",
    "82\n",
    "Пример 2: CNN за текст\n",
    "Класификација на текст (пр: дали е порака позитивна или негативна)\n",
    "Влез: Ред од зборови, претставени како низа од бројки (векторска репрезентација, \n",
    "на пример word embeddings)\n",
    "Како работи:Конволуциски филтри поминуваат преку мал број збора (на пример, \n",
    "3 последователни збора) за да фатат локални шаблони или важни комбинации (на \n",
    "пример, „многу добар“ или „не ми се допаѓа“).\n",
    "Пулинг слоевите ги собираат најважните карактеристики од целиот текст.\n",
    "На крајот, слој за класификација дава резултат (позитивен/негативен текст).\n",
    "83\n",
    "Autoencoders\n",
    "Автоенкодер е тип на feedforward \n",
    "невронска мрежа која учи да го \n",
    "„препознае“ својот влез и да го\n",
    "репродуцира истиот на излезот. Со \n",
    "други зборови, целта ѝ е да ги копира \n",
    "влезните податоци на излезот:\n",
    "Мрежата е составена од два дела:\n",
    "Енкодер (Encoder) — го зема влезот \n",
    "и го компресира (собира) во помала, \n",
    "скриена претстава (hidden layer). Ова \n",
    "е делот од влезот до скриениот слој.\n",
    "Декодер (Decoder) — го користи \n",
    "компресираниот код од скриениот слој \n",
    "за да ја реконструира оригиналната \n",
    "информација на излезот (делот од \n",
    "скриениот слој до излезот).\n",
    "Deep Autoencoders\n",
    "Длабок автоенкодер е автоенкодер кој има повеќе скриени слоеви во делот на \n",
    "енкодерот и во делот на декодерот. Тоа значи дека информацијата поминува низ \n",
    "повеќе слоеви додека се компресира и додека се реконструира.\n",
    "Gradient vanishing problem (Проблем со „изчезнување“ на градиентот)\n",
    "Кога мрежата има многу слоеви, при процесот на учење (особено при враќање \n",
    "назад на грешките, backpropagation), градиентите (кои помагаат да се прилагодат \n",
    "тежините) стануваат многу мали.\n",
    "Поради тоа, тежините во првите слоеви многу тешко се менуваат, па мрежата учи \n",
    "многу бавно или воопшто не учи добро.\n",
    "Ова е еден од главните предизвици при учење на многуслојни (длабоки) \n",
    "невронски мрежи.\n",
    "84\n",
    "На невронските мрежи им треба меморија!\n",
    "Кога работиме со секвенци (редови од податоци), како текст или говор, многу е \n",
    "важно мрежата да памети претходни информации за да разбере контекстот.\n",
    "На пример, во реченица, значењето на зборот зависи од зборовите пред него.\n",
    "Sequence modelling (Моделирање на секвенции)\n",
    "Се користи за задачи каде редоследот на податоците е важен, како текст, говор, \n",
    "или времески серии.\n",
    "Named Entity Recognition (NER) — Препознавање именувани ентитети\n",
    "Задачата е да се најдат и означат специфични зборови или фрази во текстот кои \n",
    "се име на луѓе, места, организации, и слично.\n",
    "На пример, во реченицата:\n",
    "„Apple“ може да биде име на компанија,\n",
    "„London“ е локација,\n",
    "„John“ е име на човек.\n",
    "Како работи со невронска мрежа:\n",
    "Влез: реченица претставена со низа броеви (на пример, векторски репрезентации \n",
    "на зборови).\n",
    "Мрежата ја процесира секвенцата и со меморија (на пример, со RNN или LSTM) ја \n",
    "памети претходната информација.\n",
    "Излез: за секој збор, мрежата предвидува дали тој е име на човек, локација, \n",
    "организација или нешто друго.\n",
    "Recurrent Neural Network (RNN)\n",
    "Во RNN, излезот од скриениот слој (hidden layer) не се заборавa, туку се чува \n",
    "во меморија и се користи повторно како дел од влезот за следниот чекор.\n",
    "Така, мрежата има меморија за претходните информации, што ѝ овозможува да \n",
    "работи со секвенци и редоследи.\n",
    "Како функционира:\n",
    "За секој елемент Xi од влезната низа (на пример, збор во реченица), мрежата \n",
    "генерира излез Yi, но тој излез зависи не само од тековниот влез Xi, туку и од сите \n",
    "претходни влезови x1,x2️,...,xi−1. Истата мрежа се користи повторно и повторно за \n",
    "секој елемент во низата (постојано ги обновува своите скриени состојби).\n",
    "85\n",
    "Long Short-Term Memory (LSTM)\n",
    "LSTM е посебен тип на невронска мрежа, подобрена верзија на класичниот RNN, \n",
    "која има специјални „ќелии“ (cells) со механизми за контролирање на \n",
    "информацијата.\n",
    "Овие ќелии содржат gates – “врати” кои решаваат кога да:\n",
    "- примат нова информација (input gate)\n",
    "- заборават (избришат) стара информација (forget gate)\n",
    "- испратат информација напред како излез (output gate)\n",
    "Зошто е важен LSTM?\n",
    "Класичните RNN имаат проблем со „изчезнување на градиентот“ (gradient \n",
    "vanishing) при учење на долги низи, што го отежнува паметењето на информации \n",
    "кои се далеку во минатото.\n",
    "LSTM ги решава овие проблеми така што може да памети важни информации \n",
    "подолго време и да ги заборави неважните.\n",
    "Какo работи?\n",
    "Секој LSTM ќелиски модул има 4️ главни влезови (вклучувајќи и претходната \n",
    "состојба) и 1 излез.\n",
    "Gates се базираат на сигмоидна функција (f) што одлучува дали ќе ги „отвориме“ \n",
    "или „затвориме“ вратите (вредности од 0 до 1).\n",
    "Преку овие врати, мрежата учи кога да зачува, кога да избрише, а кога да пренесе \n",
    "информација.\n",
    "86\n",
    "NLP – Natural Language Processing\n",
    "NLP е поле во КН и ВИ кое се фокусира на интеракцијата помеѓу човечкиот јазик \n",
    "и компјутерот. Целта е да се овозможи компјутерите да го разбираат, \n",
    "обработуваат и генерираат природниот јазик – да се имитира човечко разбирање.\n",
    "Примери за NLP: Spell check, Autocomplete, Speech to text, Spam filter, Related \n",
    "keywords on search engines, Siri, Alexa, Google Assistant…\n",
    "Како бизнисите може да го користат NLP: \n",
    "- Подобрување на корисничко искуство со autocomplete, spell check, autocorrect\n",
    "- Chatbots – автоматизирана поддршка\n",
    "- Анализирање на feedback од купувачите\n",
    "Седум клучни техники на NLP:\n",
    "87\n",
    "1. Sentiment Analysis (Анализа на сентимент)\n",
    "Процес на одредување на емоционалниот тон на текстот – дали е позитивен, \n",
    "негативен или неутрален.\n",
    "Пример: Рецензии на филмови, коментари на социјални мрежи.\n",
    "2. Topic Modeling (Моделирање на теми)\n",
    "Автоматско откривање на скриени теми во збирка на текстови, без потреба од \n",
    "рачно обележување.\n",
    "Пример: Групирање на вести според теми како политика, спорт, технологија.\n",
    "3. Text Categorization (Категоризација на текст)\n",
    "Доделување на текстови во претходно дефинирани категории.\n",
    "Пример: Е-пошта во спам или неспам, новост во бизнис, забава и слично.\n",
    "4. Text Clustering (Кластеризација на текст)\n",
    "Групирање на слични текстови заедно, без однапред дефинирани категории.\n",
    "Пример: Групирање на документи со слична содржина без да се знае темата \n",
    "однапред.\n",
    "5. Information Extraction (Извлекување информации)\n",
    "Автоматско вадење на структуриран податок од неструктуриран текст како факти, \n",
    "датуми, локации, имиња.\n",
    "Пример: Од новинарска статија да се извлече: Име: Петар, Локација: Скопје, \n",
    "Дата: 5️ јуни 2️02️5️.\n",
    "6. Named Entity Recognition (NER) – Препознавање на именувани ентитети\n",
    "Идентификување и класификација на посебни имиња во текстот, како лица, \n",
    "компании, места, датуми.\n",
    "Пример: Во реченицата „Apple отвори канцеларија во Берлин“ – Apple е \n",
    "организација, Берлин е локација.\n",
    "7. Relationship Extraction (Извлекување на односи)\n",
    "Наоѓање на релации меѓу ентитети во текстот.\n",
    "Пример: Од реченицата „Илон Маск е основач на Tesla“ се извлекува релација: \n",
    "Илон Маск - основач - Tesla.\n",
    "Предизвици кај NLP:\n",
    "- Зборот/реченицата може да имаат повеќе значења\n",
    "- Тешко препознавање на сарказам и иронија\n",
    "- Потребни се специфични модели за различни јазици\n",
    "- Голема димензионалност на зборови\n",
    "88\n",
    "Основни поими кај NLP:\n",
    "Синтакса – граматичка структура на реченицата.\n",
    "Семантика – значењето на зборовите/фразите/речениците.\n",
    "Part–of–Speech (POS) – идентификување на граматичката категорија на зборот \n",
    "(именка, придавка, глагол...)\n",
    "Bag–of–Words (BoW) – метод со кој се претставува колку пати се појавува \n",
    "одреден збор во реченица/текст (или бинарно – дали се појавува), игнорирајќи го \n",
    "редоследот.\n",
    "N–gram – секвенца од N зборови/знаци во текстот. (најчесто користено 2-5 )\n",
    "Unigram – секој збор одделно.\n",
    "Bigram – парови од 2️ последователни зборови\n",
    "Trigram – тројки од 3 последователни збора итн...\n",
    "Bag of words Featurization\n",
    "Создава нумерички репрезентации на текстуални податоци, при што секој збор е \n",
    "мапиран во фреквенцијата на неговата појава. Најчест начин на репрезентација \n",
    "на текст кај машинско учење.\n",
    "Едноставен пример: 1. Кучето лае.\n",
    " 2️. Мачката седи.\n",
    "Вокабулар = [кучето, лае, мачката, седи]\n",
    "Фреквенции: 1. [1, 1, 0, 0]\n",
    " 2. [0, 0, 1, 1]\n",
    "89\n",
    "Еден од предизвиците е големината на репрезентацијата.\n",
    "Бидејќи зборовите следат дистрибуција по законот на степен (power-law), \n",
    "големината на речникот расте речиси линеарно со големината на корпусот.\n",
    "Бидејќи ретките зборови не се појавуваат доволно често за да го информираат \n",
    "моделот, вообичаено е да се отфрлат невообичаените зборови што се појавуваат, \n",
    "на пример, помалку од 5️ пати.\n",
    "N – grams\n",
    "Kaj BoW се губи редоследот на зборовите и се намалува значењето на реченицата. \n",
    "Две реченици може да имаат различно значење, а ист BoW вектор.\n",
    "Карактеристики на N-grams\n",
    "Во машинското учење со текстуални податоци, честопати е корисно да се користи \n",
    "комбинација од повеќе n-грам карактеристики – на пример: униграми (единечни \n",
    "зборови), биграми (парови од последователни зборови), и триграми (тројки од \n",
    "зборови).\n",
    "Униграмите се појавуваат почесто и помагаат при откривање на послаби, \n",
    "пошироки влијанија врз текстот. Од друга страна, биграмите и триграмите можат \n",
    "да „фатат“ посилни, но поконкретни значења.\n",
    "На пример, изразот „the white house“ има значење што се разликува од збирот \n",
    "на значењата на поединечните зборови „the“, „white“, „house“, или дури и од \n",
    "биграмите „the white“ и „white house“ земени одделно.\n",
    "Кога n-grams се комбинираат со техниката BoW, тие значително ја подобруваат \n",
    "точноста на моделите за класификација и други задачи со текст.\n",
    "Типично, ваквите комбинации ги даваат следниве резултати по точност:\n",
    "3-грамови < 2-грамови < 1-грамови < 1+2-грамови < 1+2+3-грамови\n",
    "Тоа значи дека најдобри резултати се добиваат кога се користат сите три \n",
    "(униграми, биграми и триграми) заедно. Подобрувањето на точноста најчесто е \n",
    "неколку проценти – доволно значајно за реални апликации.\n",
    "Големина на N-grams\n",
    "Користењето на n-грамови носи предизвици во однос на големината на збирот на \n",
    "карактеристики (feature set). Ако првичната големина на речникот е |V|, тогаш \n",
    "бројот на можни комбинации за:\n",
    " биграми е |V|²  триграми е |V|³\n",
    "90\n",
    "Среќна околност е што природниот јазик има фреквенциска структура според \n",
    "power-law – што значи дека најголемиот број од n-грамовите што се појавуваат се \n",
    "прилично чести. \n",
    "Затоа, речник што ги содржи најчестите n-грамови ќе опфати голем дел од сите \n",
    "што ќе се сретнат во текстот.\n",
    "Пример за големини на речници со n-грамови:\n",
    " Речник со униграми: 4️0.000 зборови\n",
    " Речник со биграми: 100.000 комбинации\n",
    " Речник со триграми: 300.000 комбинации\n",
    "Со овие големини, можно е да се опфати повеќе од 80% од карактеристиките што \n",
    "се појавуваат во текстуалните податоци.\n",
    "N-grams како јазични модели\n",
    "N-grams можат да се користат за градење статистички модели на текст.\n",
    "Кога се користат на овој начин, тие се нарекуваат n-gram јазични модели (n-gram \n",
    "language models).\n",
    "Овие модели доделуваат веројатност на секој n-грам, така што збирот на \n",
    "веројатностите за сите можни n-грамови со иста големина (n) е еднаков на 1.\n",
    "Со ваков модел, може да се пресмета вкупната веројатност или веројатноста \n",
    "на појавување на одредена реченица, врз основа на веројатностите на нејзините \n",
    "поединечни n-грамови.\n",
    "Skip-grams\n",
    "Можеме да го анализираме значењето на одреден збор со тоа што ќе ги \n",
    "разгледаме контекстите во кои се појавува. Контекстот е збирот на зборови што се \n",
    "појавуваат во близина на тој збор, односно на одредено растојание: … -3, -2, -1, \n",
    "+1, +2️, +3, … во секоја реченица каде што се среќава зборот.\n",
    "Skip-gram е група од неконсеквентни зборови (со зададено растојание), кои се \n",
    "појавуваат во некоја реченица. Наместо да гледаме само зборови веднаш до \n",
    "дадениот збор, skip-gram овозможува да се вклучат и зборови кои се „прескокнати“ \n",
    "со одреден број позиции.\n",
    "На овој начин, може да се изгради BoSG – „bag of skip-grams“ репрезентација за \n",
    "секој збор, базирана на табела со skip-grams. Оваа техника овозможува побогато \n",
    "и пофлексибилно претставување на контекстот околу зборовите.\n",
    "91\n",
    "PoS (Part of Speech) – Дел на говорот\n",
    "PoS се однесува на граматичката категорија на зборовите во реченицата – на \n",
    "пример:\n",
    " именки (nouns)\n",
    " глаголи (verbs)\n",
    " придавки (adjectives)\n",
    " прилози (adverbs)\n",
    " заменки (pronouns)\n",
    " и други.\n",
    "Овие категории ни помагаат да разбереме улогата и функцијата на зборот во \n",
    "контекст на реченицата.\n",
    "PoS Tagging – Oзначување на делови на говорот\n",
    "PoS tagging претставува процес на автоматско доделување на граматичка \n",
    "категорија (т.е. дел на говорот) на секој збор во даден текст.\n",
    "Пример:\n",
    "„The dog runs fast.“\n",
    "PoS ознаки:\n",
    " The – детерминатор (DT)\n",
    " dog – именка (NN)\n",
    " runs – глагол (VBZ)\n",
    " fast – прилог (RB)\n",
    "Овој процес е важен во обработката на природен јазик (NLP) бидејќи помага во:\n",
    " разбирање на синтаксата и структурата на речениците,\n",
    " анализа на значењето,\n",
    " подобрување на машинското преведување, одговарање на прашања, и многу \n",
    "други задачи.\n",
    "92\n",
    "Named Entity Recognition (NER)\n",
    "Named Entity Recognition (NER) или препознавање на именувани ентитети е \n",
    "важна техника во обработката на природен јазик (NLP) која служи за \n",
    "препознавање и класификација на клучни информации во текстот.\n",
    "NER автоматски го пронаоѓа и класифицира текстот во одредени категории, како \n",
    "на пример:\n",
    " Имиња на лица (пр. Elon Musk)\n",
    " Имиња на организации (пр. Google, United Nations)\n",
    " Локации (пр. Skopje, Europe)\n",
    " Датуми и времиња (пр. 8 June 2025, yesterday)\n",
    " Парични износи (пр. $1000)\n",
    " Производи, настани, бренди, итн. (зависи од моделот)\n",
    "Пример:\n",
    "Текст: \"Barack Obama was born in Hawaii in 1961.\"\n",
    "NER ќе го препознае:\n",
    " Barack Obama → Person\n",
    " Hawaii → Location\n",
    " 1961 → Date\n",
    "Зошто е важен NER?\n",
    "Го структурира неструктурираниот текст.\n",
    "Се користи во пребарување на информации, чат-ботови, машинско \n",
    "преведување, анализи на документи, препознавање на субјекти во \n",
    "новинарски статии, итн.\n",
    "93\n",
    "Grammars (Граматики)\n",
    "Во компјутерската лингвистика и NLP, граматиката е формален систем на правила \n",
    "што дефинира како може да се состави валидна реченица од еден јазик.\n",
    "Најчесто се користи синтаксичка граматика, која опишува:\n",
    "кои структури се дозволени (на пр. подмет + прирок),\n",
    "како зборовите и фразите можат да се комбинираат.\n",
    "Пример од англиска граматика (во Backus–Naur формa – BNF):\n",
    "S → NP VP\n",
    "NP → Det N\n",
    "VP → V NP\n",
    "Det → \"the\" | \"a\"\n",
    "N → \"cat\" | \"dog\"\n",
    "V → \"chased\" | \"saw\"\n",
    "Ова значи дека реченицата (S) се состои од именска фраза (NP) и глаголска фраза \n",
    "(VP), итн.\n",
    "Recursion in Grammars (Рекурзија во граматики)\n",
    "Рекурзијата е важен и моќен концепт во граматиките. Таа значи дека правило \n",
    "може да се повикува самото себе, што овозможува создавање на неограничено \n",
    "долги и сложени реченици.\n",
    "Пример:\n",
    "NP → NP PP\n",
    "PP → P NP\n",
    "Овде, именската фраза (NP) може да содржи предлошка фраза (PP), која пак \n",
    "содржи нова именска фраза (NP). \n",
    "Со ова, можеме да добиеме реченици како:\n",
    "\"The cat on the mat in the room under the stairs slept.\"\n",
    "Ова покажува рекурзивна структура – фраза во фраза во фраза – што е многу \n",
    "природно за човечкиот јазик.\n",
    "94\n",
    "Зошто е ова важно во NLP?\n",
    " За парсирање на реченици (разбирање на структурата).\n",
    " Во машинско преведување и гласовно разбирање.\n",
    " За генерирање на природен јазик (на пр. во chatbots или генеративни \n",
    "системи).\n",
    "Word2Vec\n",
    "Word2Vec е популарен алгоритам во обработката на природен јазик (NLP) кој се \n",
    "користи за претставување на зборови како вектори од броеви. Оваа техника го \n",
    "претвора текстот во бројчена форма, така што машините можат полесно да го \n",
    "обработат и „разберат“ значењето на зборовите.\n",
    "Word2️Vec претставува секој збор како вектор во повеќедимензионален \n",
    "простор, така што:\n",
    " зборови со слично значење ќе бидат блиску еден до друг во векторскиот \n",
    "простор,\n",
    " а зборови со различно значење ќе бидат подалеку.\n",
    "Пример:\n",
    "„king“ – „man“ + „woman“ ≈ „queen“\n",
    "Word2️Vec користи невронска мрежа со една скриена (hidden) слој и се тренира \n",
    "на голем текстуален корпус. Постојат две главни архитектури:\n",
    "1. CBOW (Continuous Bag of Words):\n",
    "→ предвидува го средниот збор, знаејќи ги зборовите од околината.\n",
    "2. Skip-gram:\n",
    "→ зема еден збор и предвидува ги зборовите што се околу него (во \n",
    "контекстот).\n",
    "Обезбедува семантички смислени вектори (односно, „разумно“ разбирање на \n",
    "значењето на зборови),\n",
    "Може да се користи за класификација на текст, кластерирање, машинско \n",
    "преведување, препознавање на слични зборови, итн.\n",
    "95\n",
    "Ограничувања на Word2Vec\n",
    "Иако Word2️Vec добро ги пресметува сличностите меѓу зборови, има \n",
    "ограничувања:\n",
    " Најголемиот проблем е што секој збор добива само една векторска \n",
    "репрезентација, без разлика во каков контекст се појавува.\n",
    " На пример, зборот „bank“ може да значи:\n",
    "o речен брег (river bank)\n",
    "o инвестициска банка (investment bank)\n",
    "Но Word2️Vec ќе создаде единствен просечен вектор за зборот, кој не го \n",
    "претставува добро ниту едно од значењата.\n",
    "Решенија и напредок\n",
    "Поради оваа слабост, беа развиени покомплексни техники со контекстуални \n",
    "вгнездувања на зборови (Contextual Word Embeddings), како:\n",
    "o ELMo (Embeddings from Language Models) – кој го зема во предвид\n",
    "контекстот на зборот во реченицата.\n",
    "Подоцна се појавија Transformers, кои донесоа голем напредок:\n",
    "o BERT (Bidirectional Encoder Representations from Transformers)\n",
    "o GPT (Generative Pre-trained Transformer)\n",
    "Овие модели создаваат различна репрезентација на зборот во зависност од \n",
    "неговиот контекст, што овозможува подлабоко и попрецизно разбирање на јазикот.\n",
    "Transfer learning (Преносно учење)\n",
    "Во традиционалното супервизирано учење, треба да имаме многу означени \n",
    "податоци за да изградиме добар модел. Но во реалниот свет, тоа често не е \n",
    "возможно.\n",
    "Преносното учење решава овој проблем така што:\n",
    " искористува знаење стекнато од друга слична задача или домен,\n",
    " и го пренесува за да се подобри моделот за новата задача, каде што има \n",
    "малку или никакви означени податоци.\n",
    "96\n",
    "Предности на Transfer Learning\n",
    " Помага при решавање на комплексни задачи со ограничени ресурси.\n",
    " Работи и кога има многу малку означени податоци.\n",
    " Овозможува пренос на знаење од еден модел/домен на друг.\n",
    " Се смета како чекор кон вештачка општа интелигенција (AGI).\n",
    " Создава поиздржливи модели што можат да решаваат повеќе задачи.\n",
    "Тренинг на трансформери\n",
    "Трансформерите (како BERT, GPT и \n",
    "сл.) обично поминуваат низ два \n",
    "чекора:\n",
    "1. Предтренирање (pretraining) –\n",
    "на големи, необележани \n",
    "текстуални податоци.\n",
    "2. Фино прилагодување (fine\u0002tuning) – на мали, специфични \n",
    "обележани податоци за \n",
    "конкретна задача.\n",
    "Задачи што се користат во овие \n",
    "чекори:\n",
    " предвидување на следна \n",
    "реченица\n",
    " одговарање на прашања\n",
    " анализа на сентимент\n",
    " разбирање на текст (reading \n",
    "comprehension)\n",
    " парафразирање\n",
    "Примена на трансформери\n",
    "Трансформерите се најчесто \n",
    "користени во NLP. \n",
    "Популарни модели:\n",
    " GPT-2, GPT-3\n",
    " BERT\n",
    " XLNet\n",
    " RoBERTa\n",
    "Овие модели имаат примена во:\n",
    " машински превод (на пр. \n",
    "англиски ↔ македонски),\n",
    " сумирање на документи,\n",
    " генерирање на текст,\n",
    " Named Entity Recognition (NER),\n",
    " анализа на биолошки секвенци\n",
    "(на пр. ДНК анализа)\n",
    "97\n",
    "Natural Language Processing and Transformers – 2\n",
    "Јазичен модел (Language Model) претставува модел кој го опишува јазикот што \n",
    "го користи одреден ентитет или контекст, како на пример:\n",
    " Поединечен говорник или автор\n",
    " Одреден жанр (на пр. правен, медицински, социјални мрежи)\n",
    " Одредена област или домен (на пр. академско пишување, неформални \n",
    "разговори)\n",
    "Формална дефиниција: Јазичниот модел пресметува веројатност за било која \n",
    "секвенца од зборови.\n",
    "На пример, нека 𝑿 = „Елени задоцни на час“.\n",
    "Тогаш, веројатноста што ја проценува јазичниот модел е:\n",
    "P(𝑿)=P(\"Елени задоцни на час\") \n",
    "Ова значи: Колкава е веројатноста оваа реченица да се појави во јазикот на \n",
    "дадениот ентитет или домен.\n",
    "Примери: Генерирање текст (autocomplete, google translate), Класификација на \n",
    "лажни вести...\n",
    "98\n",
    "Language Modelling: Биграми (Bigrams)\n",
    "Како можеме да изградиме јазичен модел?\n",
    "Еден начин е со биграмски модел, кој е алтернативен пристап на едноставниот \n",
    "униграм модел. Наместо да ги гледаме зборовите поединечно, набљудуваме \n",
    "парови од последователни зборови.\n",
    "Биграмски модел (Bigram Model)\n",
    "Во биграмскиот модел, веројатноста на целата реченица се пресметува како \n",
    "производ од веројатностите на секој збор врз основа на претходниот збор.\n",
    "Пример:\n",
    "Нека реченицата е: „Елени задоцни на час“\n",
    "Биграмскиот модел пресметува:\n",
    "P(\"Елени задоцни на час\")=P(\"Елени\")⋅P(\"задоцни\"∣\"Елени\")⋅P(\"на\"∣\"задоцни\")\n",
    "⋅ P(\"час\"∣\"на\") \n",
    "Значи, моделот гледа секој збор во контекст на претходниот збор.\n",
    "Како функционира?\n",
    "1. Ги броиме сите парови зборови (биграми) во даден корпус (збирка на \n",
    "текстови).\n",
    "2. Се пресметува условна веројатност:\n",
    "каде w_i е тековниот збор, а w_{i-1} претходниот.\n",
    "Предности:\n",
    "Полесен и побрз од комплексни \n",
    "модели.\n",
    "Ја фаќа зависноста помеѓу зборови \n",
    "(за разлика од униграм моделот кој ги \n",
    "смета зборовите независни).\n",
    "Недостатоци:\n",
    "Не зема подлабок контекст (само еден \n",
    "претходен збор).\n",
    "Проблем со нулта веројатност ако \n",
    "парот зборови никогаш не се појавил \n",
    "→ затоа се користат техники како \n",
    "smoothing.\n",
    "99\n",
    "Language modeling: RNN\n",
    "Рекурентната невронска мрежа е тип на невронска мрежа специјално \n",
    "дизајнирана за обработка на секвенци од податоци, како што се реченици.\n",
    "Структура на RNN:\n",
    "Влезен слој (Input layer): Ги прима зборовите еден по еден како вектори (на пр. \n",
    "x₁, x₂, x₃, ...).\n",
    "Скриен слој (Hidden layer): Задржува „меморија“ за претходниот контекст преку \n",
    "повратна врска (рекуренција). Се ажурира со секој нов збор.\n",
    "Излезен слој (Output layer): Генерира веројатност за следниот збор (ŷ₁, ŷ₂,, ...)\n",
    "СИЛНИ СТРАНИ на RNN:\n",
    "Може да обработува секвенци од \n",
    "произволна должина, не само \n",
    "фиксна големина (како Feedforward \n",
    "Neural Networks).\n",
    "Задржува контекст преку скриената \n",
    "состојба – т.е., „меморија“ за тоа што \n",
    "дошло претходно.\n",
    "Истите тежини се користат за секој \n",
    "влез, што значи редоследот на \n",
    "зборовите се задржува, а бројот на \n",
    "параметри е помал.\n",
    "ПРОБЛЕМИ со RNN:\n",
    "Споро тренирање – се користи \n",
    "техника наречена Backpropagation \n",
    "Through Time (BPTT), која е \n",
    "посложена.\n",
    "Градиентите лесно исчезнуваат или \n",
    "експлодираат, особено кога \n",
    "секвенците се долги (познато како \n",
    "vanishing/exploding gradients).\n",
    "Тешко задржување на долгорочен \n",
    "контекст – RNN имаат ограничена \n",
    "способност да „се сеќаваат“ на \n",
    "зборови кои се многу далеку во \n",
    "минатото.\n",
    "Language modeling: LSTM\n",
    "LSTM е посебен тип на рекурентна невронска мрежа (RNN), дизајниран да се \n",
    "справи со долгорочни зависности во текстот — нешто со што „обичните“ (vanilla) \n",
    "RNN имаат проблем.\n",
    "Проблем кај обичните RNN: Во класичен RNN, скриената состојба (hidden state)\n",
    "се ажурира со секој нов збор — што значи дека старите информации многу лесно \n",
    "се „бришат“ и градиентите исчезнуваат при тренирање.\n",
    "100\n",
    "LSTM додава специјален дел наречен \"мемориска ќелија\" (memory cell), која \n",
    "трајно складира информации низ времето и може да:\n",
    " задржи важни информации на долг рок\n",
    " заборави непотребни информации\n",
    " додаде нови информации кога е потребно\n",
    "LSTM има неколку „врати“ (gates) – тие се мали невронски мрежи кои одлучуваат \n",
    "што да задржат, што да избришат и што да додадат:\n",
    "1. Forget Gate – одлучува што да се „заборави“ од меморијата\n",
    "2. Input Gate – одлучува што ново да се додаде\n",
    "3. Output Gate – одлучува што да се испрати како излез\n",
    "Предности:\n",
    "Речиси секогаш има подобри \n",
    "резултати од обичните (vanilla) \n",
    "RNN-и\n",
    "→ LSTM подобро ја „разбира“ \n",
    "структурата на реченицата и \n",
    "контекстот.\n",
    "Многу добро „фаќа“ долгорочни \n",
    "зависности\n",
    "→ На пример: знае дека зборовите \n",
    "„ако“ и „тогаш“ се поврзани иако се \n",
    "далеку еден од друг во реченицата.\n",
    "Недостатоци:\n",
    "Има повеќе тежини за учење \n",
    "(parameters)\n",
    "→ Поради комплексната структура со \n",
    "повеќе „врати“, LSTM има повеќе \n",
    "параметри од обичен RNN.\n",
    "Бара повеќе податоци за \n",
    "тренирање\n",
    "→ Ако имаш малку податоци, обичен \n",
    "RNN може да даде подобри резултати \n",
    "(поради помалата комплексност).\n",
    "Сè уште може да страда од \n",
    "исчезнување или експлозија на \n",
    "градиентите\n",
    "→ Иако е многу подобар од RNN, не е \n",
    "целосно имун на овие проблеми при \n",
    "долги секвенци.\n",
    "101\n",
    "Bi-directional RNN / LSTM\n",
    "Идеја: Мрежата ги чита податоците и од двете насоки\n",
    " Една RNN/LSTM оди од лево кон десно (нормално)\n",
    " Друга RNN/LSTM оди од десно кон лево (назад)\n",
    "На крај, двете информации се спојуваат и даваат поцелосен контекст.\n",
    "Предности:\n",
    "Подобро разбирање на контекстот\n",
    "(и од минатото и од иднината)\n",
    "Поголема точност во NLP задачи, \n",
    "како:\n",
    "o Тагирање на делови на \n",
    "говорот (POS tagging)\n",
    "o Named Entity Recognition \n",
    "(NER)\n",
    "Недостатоци:\n",
    "Поспоро тренирање (две мрежи \n",
    "наместо една)\n",
    "Поголема мемориска потрошувачка\n",
    "Не е погоден за стриминг податоци \n",
    "во реално време, бидејќи мора да ги \n",
    "знае и идните зборови\n",
    "o Машински превод\n",
    "o Анализа на сентимент\n",
    "Пример: „Марко не дојде на училиште затоа што беше болен.“\n",
    "Ако само гледаме од лево кон десно, до зборот „затоа што“ не знаеме дали \n",
    "причината е добра или лоша.\n",
    "Но ако ги гледаме и зборовите после „затоа што“, добиваме подобро \n",
    "разбирање.\n",
    "ELMo (Embeddings from Language Models)\n",
    "Општа идеја на ELMo:\n",
    "Целта е да се добијат богати, контекстуализирани претставувања \n",
    "(embeddings) на зборовите.\n",
    "Наместо секој збор да има фиксен вектор (како кај Word2️Vec), ELMo му дава \n",
    "различен вектор на зборот во зависност од контекстот.\n",
    "102\n",
    "ELMo: Stacked Bi-directional LSTMs\n",
    "Како работи?\n",
    "Користи двонасочни LSTM мрежи → ги зема и претходните и следните зборови\n",
    "во предвид.\n",
    "Користи повеќе слоеви (stacked) → секој слој учи сѐ покомплексни и \n",
    "поапстрактни значења.\n",
    "На крајот ги комбинира сите скриени состојби (hidden layers) од различните \n",
    "слоеви.\n",
    "Овие комбинирани претставувања се оптимизираат за конкретна задача, како:\n",
    "o Класификација на сентимент\n",
    "o Named Entity Recognition\n",
    "o Превод и сл.\n",
    "Пример: Зборот \"bank\" ќе има различна векторска претстава во:\n",
    " \"She sat by the bank of the river.\"\n",
    " \"He went to the bank to deposit money.\"\n",
    "Благодарение на контекстот, ELMo може да разбере разликата.\n",
    "Sequence-to-Sequence (Seq2️Seq) модели\n",
    "Зошто ни треба Seq2Seq?\n",
    "Кога преведуваме реченица збор по збор, често добиваме лоши преводи, бидејќи:\n",
    " Не го гледаме целиот контекст.\n",
    " Не ја земаме предвид структурата и должината на целата реченица.\n",
    "Целта на Seq2Seq моделите:\n",
    "Да се справат со:\n",
    " Превод на реченици од една јазична структура во друга\n",
    " Резимирање, автоматски дијалози, генерирање реченици, итн.\n",
    "103\n",
    "Како работи Seq2Seq?\n",
    "Моделот се состои од два дела:\n",
    "1. Енкодер RNN (Encoder)\n",
    "Го чита целото влезно речење \n",
    "(на пример, на англиски).\n",
    "Ја претвора целата секвенца во \n",
    "една контекстуална векторска \n",
    "претстава (се нарекува context \n",
    "vector или hidden state).\n",
    "Пример: „I am hungry“ се \n",
    "претвора во context_vector\n",
    "2. Декодер RNN (Decoder)\n",
    "Почнува од context_vector и \n",
    "генерира излезно речење (на \n",
    "пример, на француски), збор по \n",
    "збор.\n",
    "Генерира зборови сè додека не \n",
    "стигне до посебен симбол \n",
    "<EOS> (end of sequence).\n",
    "При тестирање, секој излезен \n",
    "збор (ŷᵢ) станува влез за \n",
    "следниот чекор.\n",
    "Тренирање (Training):\n",
    "Се користи ист метод како кај RNN: Backpropagation Through Time (BPTT).\n",
    "Се мери грешката на декодерот (на пример, колку далеку е преводот од \n",
    "вистинскиот).\n",
    "Се ажурираат тежините на двете мрежи – од крајот на декодерот до почетокот на \n",
    "енкодерот.\n",
    "Предности:\n",
    " Може да работи со секвенци од \n",
    "различна должина (на пример, 5️ \n",
    "зборови влез → 7 зборови излез)\n",
    " Може да фати долгорочни \n",
    "зависности меѓу зборовите\n",
    " Флексибилен е за превод, \n",
    "дијалог, резиме, chatbot, итн.\n",
    "Ограничување:\n",
    "Се потпира на една единствена \n",
    "скриена состојба како „меморија“ –\n",
    "што не е секогаш доволно (ова ќе го \n",
    "надмине attention механизмот кај \n",
    "модерните модели како \n",
    "Transformer/BERT).\n",
    "104\n",
    "Attention механизам во Seq2️Seq модели\n",
    "Што ако декодерот може селективно да се фокусира на делови од влезната \n",
    "секвенца при секој чекор?\n",
    "Attention овозможува декодерот да доделува различни тежини на сите скриени \n",
    "состојби од енкодерот.\n",
    "Овој механизам создава динамична, тежинска комбинација од енкодер излезите, \n",
    "со што го насочува декодерот да ја нагласи најрелевантната информација.\n",
    "Со тоа што се фокусира на вистинскиот контекст во вистинско време, attention\n",
    "помага да се:\n",
    " Обработуваат долги секвенци поефикасно\n",
    " Подобри точноста кај задачи како превод и резимирање\n",
    " Разјасни нејаснотии преку искористување на целиот влезен контекст\n",
    "Attention значително ги подобрува резултатите на Seq2Seq моделите\n",
    "105\n",
    "Модел Контекст\n",
    "Справувањ\n",
    "е со долги \n",
    "секвенци\n",
    "Мемориј\n",
    "а/ \n",
    "Состојба\n",
    "Тип на \n",
    "излез\n",
    "Предности Недостатоци\n",
    "Биграми\n",
    "1 збор \n",
    "(претходе\n",
    "н збор)\n",
    "Лошо Нема\n",
    "Статисти\u0002чка \n",
    "веројатн\n",
    "о-ст\n",
    "Едноставен, брз \n",
    "за тренирање\n",
    "Игнорира долгорочен \n",
    "контекст, не разбира \n",
    "значење\n",
    "RNN\n",
    "Цела \n",
    "претходна \n",
    "секвенца\n",
    "Ограниче\u0002но\n",
    "Скриена \n",
    "состојба h\n",
    "По еден \n",
    "збор во \n",
    "секвенца\n",
    "Може да \n",
    "обработува \n",
    "секвенци од \n",
    "променлива \n",
    "должина\n",
    "Проблеми со \n",
    "исчезнувачки/експлодира\n",
    "чки градиенти\n",
    "LSTM\n",
    "Цела \n",
    "претходна \n",
    "секвенца\n",
    "Добро\n",
    "Скриена \n",
    "состојба \n",
    "h, \n",
    "меморија \n",
    "c\n",
    "По еден \n",
    "збор во \n",
    "секвенца\n",
    "Подобро \n",
    "справување со \n",
    "долгорочни \n",
    "зависимости\n",
    "Повеќе параметри, бара \n",
    "повеќе податоци и време \n",
    "за тренирање\n",
    "ELMo\n",
    "Цела \n",
    "реченица \n",
    "(би\u0002дирекцио\u0002нално)\n",
    "Одлично\n",
    "Stack од \n",
    "bi-LSTM + \n",
    "тежинска \n",
    "комбина\u0002ција\n",
    "Векторск\n",
    "а \n",
    "претстав\n",
    "а на \n",
    "зборови\n",
    "Богати, \n",
    "контекстуализира\n",
    "ни вгнездувања\n",
    "Голем, не е генеративен, \n",
    "скап за тренирање\n",
    "Seq2Seq\n",
    "Цела \n",
    "влезна \n",
    "секвенца\n",
    "Добро\n",
    "Енкодер\u0002декодер, \n",
    "фиксен \n",
    "контексте\n",
    "н вектор\n",
    "Цела \n",
    "излезна \n",
    "секвенца \n",
    "(разл. \n",
    "должина\n",
    ")\n",
    "Поддржува \n",
    "влез/излез од \n",
    "различна \n",
    "должина, корисен \n",
    "за превод, \n",
    "дијалози и сл.\n",
    "Целиот контекст е \n",
    "компримиран во еден \n",
    "вектор (освен ако нема \n",
    "attention)\n",
    "Attention\n",
    "Цела \n",
    "влезна \n",
    "секвенца \n",
    "(динамичк\n",
    "и фокус)\n",
    "Многу \n",
    "добро\n",
    "Варијаби\u0002лна \n",
    "тежинска \n",
    "комбина\u0002ција од \n",
    "состојби\n",
    "Подобре\n",
    "н \n",
    "Seq2Seq \n",
    "излез\n",
    "Подобрување на \n",
    "долгорочен \n",
    "контекст, \n",
    "визуализација на \n",
    "вниманието\n",
    "Се користи како додаток; \n",
    "самостојно не е модел\n",
    "Transform\n",
    "er\n",
    "Цела \n",
    "секвенца, \n",
    "парал\u0002елно\n",
    "Одлично\n",
    "Self\u0002attention \n",
    "наместо \n",
    "рекурзија\n",
    "Цела \n",
    "секвенца \n",
    "истовре\u0002мено \n",
    "(маскир\u0002ана)\n",
    "Брз, држи \n",
    "глобален \n",
    "контекст, state-of\u0002the-art во NLP\n",
    "Бара многу ресурси за \n",
    "тренирање, комплексен\n",
    "106\n",
    "Што е трансформер?\n",
    "Трансформер е вид на модел за обработка на податоци, најчесто користен за јазик \n",
    "(текст), кој го подобрува работењето на претходните модели како што се RNN и \n",
    "LSTM.\n",
    "Зошто е важен трансформерот?\n",
    "Проблемите кај RNN и слични модели се:\n",
    " Тие тешко се справуваат со „долги врски“ во текстот (на пример, ако нешто е \n",
    "кажано на почетокот, да се поврзе со нешто на крајот од текстот).\n",
    " Тренингот трае долго затоа што секој дел од влезот се обработува \n",
    "последователно (еден по еден).\n",
    " Тешко е да се искористи моќта на современите компјутери кои сакаат да \n",
    "работат паралелно.\n",
    "Трансформерот решава овие проблеми.\n",
    "Како работи трансформерот?\n",
    "Влез:\n",
    " Влезот е текст (на пример, реченица).\n",
    " Текстот се претвора во броеви (векторски претстави на зборови), наречени \n",
    "ембединг (embedding).\n",
    "Клучна идеја: „Attention“ (внимание)\n",
    " Трансформерот користи нешто што се вика „self-attention“ или само\u0002внимание.\n",
    " Ова значи дека моделот гледа на секој збор во реченицата и го споредува со \n",
    "сите други зборови за да разбере колку тие се поврзани или важни едни за \n",
    "други.\n",
    " На пример, во реченицата „Кучето што лаеше беше големо“, зборот „кучето“ \n",
    "е поврзан со „лаеше“ и „големо“. Моделот ќе обрне внимание на овие врски.\n",
    "Паралелна обработка:\n",
    " За разлика од RNN кој обработува збор по збор, трансформерот гледа цела \n",
    "реченица одеднаш и ги обработува сите зборови паралелно.\n",
    " Ова го прави тренирањето многу побрзо.\n",
    "107\n",
    "Архитектура\n",
    "Трансформерот има два дела:\n",
    "1. Encoder (Енкодер)\n",
    "o Го зема влезниот текст и го претвора во внатрешна претстава (скриени \n",
    "вектори) што ги содржи сите важни информации и врски меѓу \n",
    "зборовите.\n",
    "o Секој слој во енкодерот ги користи self-attention механизмите и мали \n",
    "вештини на учење (feed-forward мрежи) за подобро разбирање.\n",
    "2. Decoder (Декодер)\n",
    "o Го користи резултатот од енкодерот за да креира излез (на пример, \n",
    "превод на друг јазик или продолжување на текст).\n",
    "o Декодерот исто така користи attention, но има и механизам за \n",
    "„последователно“ креирање на зборови (генерира збор по збор).\n",
    "Инпут и аутпут пример\n",
    "Ако сакаме да го преведеме „Јас сакам јаболко“ на англиски:\n",
    " Инпут: „Јас сакам јаболко“ (кодерот го обработува целиот текст одеднаш)\n",
    " Внатрешна претстава: трансформерот разбира дека „Јас“ е субјект, „сакам“ \n",
    "е глагол, „јаболко“ е објект.\n",
    " Аутпут: „I want an apple“ (декодерот го генерира излезот збор по збор, \n",
    "користејќи ја информацијата од кодерот).\n",
    "Зошто трансформерите се толку моќни?\n",
    " Ги наоѓаат најважните врски во текстот без разлика на должината.\n",
    " Можат да се тренираат побрзо затоа што користат паралелна обработка.\n",
    " Се основа на вниманието кое е многу флексибилно и ја прави машината да \n",
    "„разбира“ подобро контекстот.\n",
    " Поради овие причини, денес најмодерните јазични модели како GPT, BERT, \n",
    "T5️ се базираат на трансформери.\n",
    "108\n",
    "GPT-2️ и BERT\n",
    "Трансформерот има два дела:\n",
    " Encoder (енкодер) кој ја обработува влезната информација (на пример текст)\n",
    " Decoder (декодер) кој создава излез (на пример превод или генерирање на \n",
    "текст) користејќи ја информацијата од кодерот.\n",
    "Ако немаме влезен текст, туку само сакаме да генерираме „следен збор“ еден \n",
    "по еден (на пример автоматско пишување на реченица), можеме да го отстраниме \n",
    "енкодерот и да користиме само декодерот за тоа. Оваа архитектура ја користи GPT \n",
    "(Generative Pre-trained Transformer).\n",
    "Ако сакаме само да обучиме јазичен модел кој ќе ни помогне во други задачи\n",
    "(на пример препознавање на значење во текст), ни не треба декодерот, туку само \n",
    "енкодерот. Оваа верзија е BERT (Bidirectional Encoder Representations from \n",
    "Transformers).\n",
    "Трансформер = кодер + декодер\n",
    "GPT = само декодер (генерира текст последователно)\n",
    "BERT = само енкодер (разбирање на текст)\n",
    "109\n",
    "Unsupervised learning\n",
    "Unsupervised Learning е област од машинското учење каде немаме обележани \n",
    "излезни податоци. Овде целта е да се најдат скриени шеми, структури или \n",
    "кластери во податоците. Алгоритмите од овој тип се користат кога сакаме да го \n",
    "анализираме начинот на кој податоците природно се групираат или да намалиме \n",
    "димензионалност, без претходно да знаеме како треба да изгледаат резултатите.\n",
    "Кластеринг е основна техника во unsupervised learning. Тоа претставува процес \n",
    "на групирање на слични објекти така што објектите во истата група (кластер) се \n",
    "што е можно послични еден со друг, а различни од објектите во другите групи. \n",
    "Примери за примена на кластеринг се групирање на документи, слики, биолошки \n",
    "примероци, или кориснички однесувања.\n",
    "Постојат неколку главни типови на кластеринг: partitioning (како што е K\u0002means), hierarchical (агломеративен и дивизивен), и density-based (како DBSCAN). \n",
    "Секој од нив има различна примена и се однесува поинаку во присуство на шум, \n",
    "неправилни форми или кластери со различна густина.\n",
    "Во partitioning кластерингот, како K-means, однапред одредуваме колку \n",
    "кластери сакаме да добиеме. Секој кластер има центар, наречен центроид, и \n",
    "секоја точка се доделува на најблискиот центроид. Процесот се повторува: се \n",
    "пресметуваат новите центроиди според новите групи, сè додека кластерите не \n",
    "престанат да се менуваат. Овој алгоритам е едноставен, но може да биде \n",
    "чувствителен на иницијалните вредности и не работи добро со кластери од \n",
    "различна големина, форма или густина.\n",
    "За да се подобри ова, се користи K-means++ методот за подобра \n",
    "иницијализација на центроидите. Наместо случаен избор, се избираат точки кои се \n",
    "најдалеку од веќе избраните, со што се подобрува стабилноста и точноста на \n",
    "алгоритмот.\n",
    "Hierarchical clustering гради хиерархија од кластери и обично се претставува со \n",
    "dendrogram (дрвовидна структура). Најчест метод е агломеративниот (bottom-up), \n",
    "каде секоја точка започнува како посебен кластер, а потоа се спојуваат најблиските \n",
    "парови. Различни методи постојат за мерење на „блискост“ меѓу кластери: single\u0002link (најблиска точка), complete-link (најдалечна точка), average-link (просек на сите \n",
    "растојанија), и Ward's метод (зголемување на вкупна грешка).\n",
    "Овој тип на кластеринг има предност што не мора однапред да се знае бројот на \n",
    "кластери, но има и недостатоци како висока временска и просторна сложеност и \n",
    "осетливост на шумови.\n",
    "110\n",
    "Density-based clustering (DBSCAN) дефинира кластер како област со висока \n",
    "густина. Секој кластер се состои од core точки (со доволно блиски соседи), border \n",
    "точки (во близина на core, но не и самите core), и noise точки (точки што не \n",
    "припаѓаат на ниту еден кластер). Алгоритмот работи без да бара однапред број на \n",
    "кластери и е многу добар со неправилни форми и outliers. Но, има проблеми при \n",
    "различна густина или висока димензионалност на податоците.\n",
    "За да се оцени квалитетот на кластерите, се користат различни метрики. На \n",
    "пример, SSE (Sum of Squared Errors) мери колку точките се блиску до центарот на \n",
    "нивниот кластер. Silhouette Score мери сличност на точка со својот кластер во \n",
    "однос на другите. Понекогаш се користат и визуелизации или корелација на \n",
    "similarity матрици.\n",
    "Autoencoders се невронски мрежи кои учат да го реплицираат влезот како излез. \n",
    "Тие го компресираат влезот во мала латентна претстава (bottleneck) и потоа ја \n",
    "реконструираат оригиналната слика или податок. Се користат за репрезентациско \n",
    "учење, детекција на аномалии и генеративно моделирање. Основниот autoencoder \n",
    "има encoder, bottleneck и decoder. Подобри варијанти вклучуваат deep \n",
    "autoencoders, convolutional autoencoders (за слики), overcomplete и variational \n",
    "autoencoders (VAE), кои користат статистички модели за подобра генерација.\n",
    "Autoencoders можат да се обучуваат на податоци без ознаки и се добар пример за \n",
    "self-supervised learning, бидејќи целта е да се научи претставување од самиот \n",
    "податок. Latent просторот што се добива може да се користи за кластеринг или \n",
    "други задачи.\n",
    "Во практика, unsupervised learning се користи кога имаме многу податоци, но \n",
    "малку знаеме за нивната структура. Техниките како кластеринг и автоенкодери \n",
    "овозможуваат разбирање, визуелизација и дури и генерација на нови податоци.\n",
    "Клучни поими:\n",
    " Unsupervised learning – тип на машинско учење каде што нема излезни \n",
    "ознаки; моделот сам наоѓа шеми во податоците.\n",
    " Clustering – процес на групирање слични податоци во кластери.\n",
    " K-means – алгоритам за кластеринг со однапред зададен број на кластери; \n",
    "се базира на растојание до центроиди.\n",
    " K-means++ – подобрување на K-means преку паметен избор на почетни \n",
    "центроиди.\n",
    "111\n",
    " Hierarchical clustering – градење хиерархија на кластери, прикажани преку \n",
    "дендрограм.\n",
    " Agglomerative clustering – тип на hierarchical кластеринг што започнува со \n",
    "поединечни точки и ги спојува.\n",
    " DBSCAN – алгоритам базиран на густина, што не бара однапред зададен \n",
    "број на кластери и открива шумови.\n",
    " Core, Border, Noise points – категории во DBSCAN.\n",
    " SSE (Sum of Squared Errors) – метрика за мерење на компактноста на \n",
    "кластери.\n",
    " Silhouette Score – метрика за проценка на квалитетот на кластеринг.\n",
    " Autoencoder – невронска мрежа што учи да го репродуцира влезот и создава \n",
    "скриена претстава (latent space).\n",
    " Latent space – компактна, скриена репрезентација на влезните податоци.\n",
    " Variational Autoencoder (VAE) – верзија на автоенкодер што моделира \n",
    "латентниот простор како веројатносна распределба."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
