{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d919d0b-7ce9-4874-acdd-339df028ade6",
   "metadata": {},
   "source": [
    "Зошто Attention ги подобрува seq2seq моделите? \n",
    "- Attention механизмот ги подобрува seq2seq моделите затоа што му овозможува на моделот динамично да се фокусира на најрелевантните делови од влезната секвенца при секој чекор на генерирање.\n",
    "- Кај класичните seq2seq модели без attention, целата влезна секвенца се компресира во еден фиксeн вектор, што доведува до губење на информации, особено кај долги реченици. Attention го решава овој проблем така што декодерот, наместо да се потпира на еден вектор, директно пристапува до сите скриени состојби на енкодерот и за секој излезен збор пресметува тежини кои покажуваат колку е важен секој влезен збор во тој момент.\n",
    "- Со тоа се зачувуваар повеќе информации од влезот, се подобрува разбирањето на долги секвенци, се зголемува точноста на предодот и генерирањето на резултати."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc32265-5843-4e44-a4fd-7b2b2d6364c8",
   "metadata": {},
   "source": [
    "Разлика помеѓу RNN и LSTM?\n",
    "- RNN (Reccurent Neural Network) обработува секвенци со пренесување на скриена состојба од чекор во чекор. Иако може да моделира редослед, кај подолги секвенци често се јавува проблемот на исчезнување и експлодирачки градиенти, што го отежнува учењето долгорочни зависности.\n",
    "- LSTM (Long Short-Term Memory) е специјален тип на RNN кој воведува мемориски ќелии и порти. Овие порти контролираат кои информации да се задржат, а кои да се заборават, со што LSTM значително подобро ги зачувува важните информации низ времето.\n",
    "- LSTM е развиен токму за да ги надмине ограничувањата на класичните RNN модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc752403-df05-49aa-956e-065b8f9bec0f",
   "metadata": {},
   "source": [
    "Што е точно за Feed Forward Neural Network?\n",
    "- FFNN е најосновниот тип на невронска мрежа кај кој информацијата се движи во една насока (од влезниот слој, преку еден или повеќе скриени слоеви до изелзниот слој.\n",
    "- Нема повратни врски, секој неврон е поврзан со сите неврони од следниот слој, користи активациски функции се обучува со backpropagation и градиентско спуштање.\n",
    "- Едноставна структура, брзо тренирање и лесна имплементација."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd2c7f-e92e-46f9-a60c-ca97b9abf6c2",
   "metadata": {},
   "source": [
    "Која е улогата на feed forward neural networks во transformer архитектурата?\n",
    "- Во transformer архитектурата, Feed Forward Neural Networks (FFNN) имаат клучна улога во обработката на информации по секој attention слој.\n",
    "- Постојат по секој attention слој и се применуваат одделно на секој токен.\n",
    "- Се состојат од два dense слоја со активациска функција помеѓу нив (прв слој го зголемува бројот на димензии, вториот слој го враќа на оргиналната димензија)\n",
    "- Целта е да се аплицира нелинеарна трансформација на секој токен по внимателно комбинирање на информации од attention слојот.\n",
    "- FFNN овозможува на моделот да учи сложени обрасци и комбинации на контекстуалните информации, кои не можат да се фатат само со attention.\n",
    "- Во трансформерите, FFNN е како „локален обработувач“ за секој токен кој по дополнителната обработка од attention слоевите ја надградува репрезентацијата со нелинеарни трансформации, овозможувајќи подобро учење на сложени шаблони."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e6e4e7-de0e-412b-872c-9129a10bda5a",
   "metadata": {},
   "source": [
    "Објасни за multi-head attention кај трансформери и зошто positional encoding е битен кај трансформери?\n",
    "- Multi-head attention кај трансформерите овозможува моделот истовремено да обрнува  внимание на различни делови од влезната секвенца. Наместо еден attention механизам, се користат повеќе „глави“, при што секоја учи различни односи (на пр. синтаксички, семантички или далечни зависности). Ова му дава на моделот побогато и попрецизно разбирање на контекстот.\n",
    "- Positional encoding е битен затоа што трансформерите немаат вградена информација за редоследот на зборовите (за разлика од RNN). Со positional encoding се додава информација за позицијата на секој елемент во секвенцата, за моделот да знае кој збор е прв, втор, итн., и да го разбере значењето зависно од редоследот."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07d771-401c-431b-be7b-35b3a36c787f",
   "metadata": {},
   "source": [
    "Attention mask се користи кај трансформерите за да се контролира на кои делови од влезот моделот смее да обрнува внимание.\n",
    "- Attention mask кажува што моделот смее, а што не смее да „види“ при пресметка на attention\n",
    "- Најчести улоги:\n",
    "Padding mask: ги игнорира празните (padding) токени за тие да не влијаат на пресметката.\n",
    "Look-ahead / causal mask: спречува моделот да „гледа во иднината“, важно кај јазични модели при генерирање текст.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b3eb72-6bb8-4431-9d59-d63a7a15355e",
   "metadata": {},
   "source": [
    "Споредба помеѓу  LSTM и BiLSTM?\n",
    "- LSTM (Long Short-Term Memory) – обработува секвенци во една насока (од почеток до крај). Користи мемориски ќелии за да задржи важни инфромации од минатото, што е корисно кога контекстот главно доаѓа од предходните елементи.\n",
    "- BiLSTM (Bidirectional LSTM) – обработува секвенци во две насоки (напред и назад). Секој елемент има контекст и од минатото и од иднината, што дава поточно разбирање на целата секвенца. Често е попрецизен, покомплексен и побавен.\n",
    "- LSTM е добар за реално-временски задачи, додека BiLSTM е подобар кога целиот контекст на секвенцата е достапен (пр. Анализа на текст).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c75c9c4-7f76-41d4-b75c-fe958afcdcf6",
   "metadata": {},
   "source": [
    "Споредба на BiLSTM и Transformers?\n",
    "- BiLSTM ги обработува податоците последователно во две насоки, од почеток кон крај и обратно, што му овозможува на моделот да користи и минати и иден контекст при носење на одлуки. Овај пристап е ефикасен кај пократки секвенци и помали сетови на податоци, но поради секвенцијалната природа е потежок за паралелизација и побавен за тренеирање, особено кај подолги текстови. Иако  LSTM механизмот помага во зачувување на информацијата, долгорочните зависности може постепено да ослабат. \n",
    "- Додека трансформерите ја обработуваат секвенцата одеднаш, без зависност од редоследна пресметка и се базираат на self-attention механизам, со што овозможуваат секој елемент во секвенцата дирекно да “внимава” на сите други елементи. Ова овозможува многу подобро моделирање на долгорочни зависности и целосно паралелна обработка.Трансформерите се покомплексни и бараат повеќе податоци и пресметковни ресурси, но токму поради нивната скалабилност и висока точност денес се стандард во современите NLP апликации.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6046ee81-c124-4387-8f7e-5ff7f8fdcd32",
   "metadata": {},
   "source": [
    "Како CNN, Transformers и LSTM се разликуваат од традиционалните алгоритми при обработка на низи од зборови, да се споредат, предности, карактеристики и се останато од што се составени и итн кои како работат со NLP и кои од овие е најдобар?\n",
    "- При обработка на NLP, cnn,lstm и transformers значително се разликуват од традиционалните алгоритми по тоа што учат значење и контекст автоматски, наместо рачно да извлекуваат карактеристики.\n",
    "- Традиционалните алгоритми го претставуваат текстот како вектор од фреквенции, при што редоследот и значењето на зборовите најчесто се губат. Тие се едноставни, брзи и лесни за интерпретација, но не можат да разберат контекст, синоними или долгорочни зависности.\n",
    "- CNN во nlp користат конволуциски филтри врз embeddings на зборови за да препознаат локални шаблони, како фрази или клучни комбинации на зборови. Тие се брзи, ефикасни и добри за задачи како класификација на текст и sentiment анализа, но имаат ограничена способност за разбирање на глобален контекст.\n",
    "- LSTM е рекурентна мрежа дизајнирана да работи со сквенци и да задржува информација од претходните зборови преку мемориски ќелии. Таа е многу подобра од традиционалните модели во фаќање контекст и редослед, но поради секвенцијалната обработка е побавна и потешко се склаира кај долги текстови.\n",
    "- Трансформерите користат self-attention механизми и positional encoding, со што секој збор може дирекно да обрне внимани на сите други зборви во реченицата. Ова овозможува паралелна обработка, одлично фаќање долгорочни зависности и највисоки перформанси во модерниот NLP. Тие се составени од attenition layers, feed-forward networks and positional encoding, но бараат големи колочини податоци и пресметковни ресурси. \n",
    "- Најдобар модел денес за NLP се трансформерите, особено кај комплексни задачи, додека LSTM и CNN се уште имаат предност кај помали проблеми и ограничени ресурси."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c6f4a1-4d16-4dd6-a7e1-ca748e0d075e",
   "metadata": {},
   "source": [
    "Што е точно за lags?\n",
    "- Lag features ги користат вредностите од минатото (n чекори наназад) за да се предвиди сегашноста или иднината. Клучни се за временски серии. Премногу lags mоже да доведе до overfitting. Lags се начин минатите вредности да се вклучат како карактеристики за подобро предвидување."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d4b3d9-c5cf-43a2-ae22-129259b6767a",
   "metadata": {},
   "source": [
    "Што значи return_sequences=True во LSTM? \n",
    "- Ова значи дека LSTM ќе врати целата секвенца од скриени состојби за секој временски чекор, наместо само последната. Ова е важно кога следниот слој (на пример друг LSTM или TimeDistributed Dense) има потреба од излез за секој чекор во времето. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bded5ac0-019d-4d04-8a4e-d91dfbd89754",
   "metadata": {},
   "source": [
    "Тренирање на модел со финансиски и временски податоци од берза, кој модел е најдобар да се употреби?\n",
    "- LSTM / BiLSTM – многу соодветни за берзански податоци бидејќи фаќаат долгорочни временски зависности и нелинеарни обрасци.\n",
    "- Transformers (Time Series Transformers) – најмоќни за големи сетови податоци и комплексни шаблони, но бараат многу ресурси.\n",
    "- Најпрактичен избор во реални услови е LSTM, бидејќи балансира помеѓу точност и комплексност, додека трансформерите се најмоќни кога има доволно податоци и ресурси."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b00cd-fc36-4cce-a01e-d5c6ed93004f",
   "metadata": {},
   "source": [
    "Како DBSCAN се справува со overfitting наспроти K-means? Наведи ситуации каде DBSCAN би довел до подобри кластери него k-means?\n",
    "- K-means бара однапред да се зададе бројот на кластери (K). Ако K е преголем, алгоритмот може да „пренавучи“ (overfit) – создава премногу мали кластери што го следат шумот во податоците и е чувствителен на outliers.\n",
    "- DBSCAN, од друга страна, не бара број на кластери. Тој групира точки според густина и експлицитно ги означува изолираните точки како шум (outliers). Поради тоа, DBSCAN е поприродно отпорен на overfitting, бидејќи не се обидува да формира кластери таму каде што нема доволна густина на податоци.\n",
    "\n",
    "- 2. Нелинерани или неправлини облици на кластери, пр. ако податоците се во облик на прстен или пола месечина, к-минс очекува сферичен облик, па ќе ги раздели неправилно додека DBSCAN ќе ги групира правилно по густина.\n",
    "- Податоци со различна густина, ако еден кластер е густ а друг е редок к-минс често ќе ги меша или поделува неправилно додека DBSCAN ги открива и ретките кластери без да ги спои со другите.\n",
    "- Присуство на шум и аутлаери, к-минс ќе се обиде да ги вклучи сите точки вклучуваќи ги и аутлаерите што ја нарушува позицијата на центроидите, додека DBSCAN ги означува аутлаерите како шум и не ги вчитува во кластерите.\n",
    "- К-минс бара однапред да се определи бројот на кластери, што н понекогаш е непрактично, додека DBSCAN автоматски одредува број на кластери според густината."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337ba18-704c-4acc-83c5-935e78517a12",
   "metadata": {},
   "source": [
    "Како трансформерите ги енкодираат зборовите?\n",
    "- Трансформерите ги енкодираат зборовите преку комбинација на embedding слојот и positional encoding.\n",
    "- Најпрво секој збор се мапира во embedding вектор со фиксна димензија, кој ја аноси неговата семантичка информација. Потоа, бидеќи трансформерите сами по себе немаат информација за редослед, на овие embeddings им се додава positional encoding, кој ја венсува инфромацијата за позицијата на зборот во реченицата.\n",
    "- Овие збогатени вектори потоа влегуваат во self-attention слоевите, каде што секој збор \"внимава\" на сите други зборови во реченицата и го прилагодува своето значење според контекстот."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8ac3b2-9c44-4942-b86b-fedc9b5c4a48",
   "metadata": {},
   "source": [
    "Seq2Seq (Sequence-to-Sequence) позитивни и негативни страни?\n",
    "- sequence-to-sequence моделите се најпогодни за задачи каде од еден влез треба да се создаде нов текст, како што се машински превод, автоматско резимирање и генеративно одговарање на прашања. Добро го моделираат редоследот на податоците.\n",
    "- Недостатоци- моделите без attention не памтат долго, потешко учат кај долги секвенци и имаат бавно тренирање."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdd6b71-2703-43c3-b49d-b929313bff74",
   "metadata": {},
   "source": [
    "Кој алгоритам креира празни кластери и зошто?\n",
    "- Алгоритмот што може да креира празни кластери е K-means.\n",
    "- K-means започнува со случајно иницијализирани центроиди. Ако никој податок не е бајблизок до одреден центроид за време на итерација, тој кластер останува празен.\n",
    "- Празните кластери се резултат на лоша иницијализација или неправилна распределба на податоците. \n",
    "- Најдобро решение е тие кластери да се одстранат или да се воведе променлив избор на центроидите(к-меанс++)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5233ab9e-686c-4a85-8c4c-f92bc93547db",
   "metadata": {},
   "source": [
    "Еднонасочна vs двонасочна RNN (Unidirectional vs. Bidirectional RNN)?\n",
    "- Еднонасочна обработува секвенца само во една насока, обично од почеток кон крај. Се користи информации само од предходните елементи, применлива е кога идните податоци не се достапни.\n",
    "- Двонасочна има два паралелни РНН слоја - еден оди напред друг назад, секој елемент има контекст и од минатото и од иднината, подобра е за задачи каде целиот текст е достапен.\n",
    "- РНН може да биде длабока, тоа значи дека име повеќе слоеви РНН еден врз друг. тие можат да учат постепени и сложени обрасци, но се потепки за тренирање."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b0ad8-586c-42f2-ba28-450cb87760ad",
   "metadata": {},
   "source": [
    "Кога хиерархиско кластерирање дава лоши перформанси?\n",
    "- Големи сетови на податоци, имаат висока комплексност па стануваат бавни и потешки за меморија.\n",
    "- Нелинеарна или комплексна структура на податоците, кластерите не се компактни или имаат различни густини и облици, алгоритмот може да ги спои неправилно\n",
    "- Шумливи податоци и аутлаери - чувствително на шум бидеќи едно неправилно спојување или аутлаер може да ја промени хиерархијата\n",
    "- Погрешен избор на мерна сличност- можат да создадат лоши резултати ако не одговараат на природната распределба на податоците.\n",
    "- Хиерархиското кластерирање е добро за мали, чисти и добро поделени податоци, но не е ефикасно за големи, шумливи или комплексни сетови."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f733d6-6ae5-4838-942a-9e9c40f55c6d",
   "metadata": {},
   "source": [
    "Зошто нормализација е важна за хиерархиско кластерирање?\n",
    "- Нормализацијата е важна за хиерархиско кластерирање затоа што овој метод се базира на мерки на растојание (на пример, Euclidean, Manhattan) за да ги одреди сличностите помеѓу податоците.\n",
    "- Подеднакво влијание на сите карактеристики - ако една карактеристика има поголема скала, таа ќе доминира во пресметката на растојанието. Нормализацијата го избегнува тоа.\n",
    "- Пр. Ако имаме Вкупно постигнати голови во една ракометна сезона (0-600) и број на црвени картони во таа сезона (0-20), без нормализација поените ќе доминираат при пресметка на растојанието. Алгоритмот ќе ги класифицира играчите само според головите, а не според добиени црвени картони.\n",
    "- Точни кластери - без нормализација, растојанието ќе биде погрешно, што може да доведе до неправилни псојувања.\n",
    "- Нормализираните податоци ја прават хиерархијата постојана и реплицирана, без да зависат од единиците на мерка."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48572c2d-bcaf-47c7-bc2d-32fec74665ee",
   "metadata": {},
   "source": [
    "Како вработен во финтек компанија добивате задача да ги оцените вестите дали се спортски, политички, финансиски, економски или забавни. Притоа, добивате насоки да користите LSTM невронски мрежи за да го изградите моделот. За каков тип на \n",
    "NLP таск (задача) станува збор? Наведете ги сите потребни претпроцесирачки чекори, архитектурата на моделот кој \n",
    "што би го користеле, процесот на тренирање и евалуација, како и техника со која би го избегнале overfitting. \n",
    "- Станува збор за мултикласна класификација на текст (Multi-class Text \n",
    "Classification)\n",
    "- Претпроцесирање: Чистење, токенизација, embeddings, padding, label encoding.\n",
    "- Модел: Embedding → LSTM → Dropout → Dense(softmax).\n",
    "- Тренирање: Crossentropy loss, Adam, метрики: Accuracy + F1.\n",
    "- Anti-overfitting: Dropout, Early Stopping, Regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e2313-c425-4658-9a97-89e2ff4351fd",
   "metadata": {},
   "source": [
    "Назадна пропагација (backpropagation) \n",
    "- Е важна математичка алатка за подобрување на точноста на предвидувањата во рударството на податоци и машинското учење.\n",
    "Во суштина, назадната пропагација e алгоритам што се користи за брзо пресметување на деривати (derivates).\n",
    "Вештачките невронски мрежи ја користат назадната пропагација како алгоритам за учење за да пресметаат градиентно спуштање во однос на тежините. Посакуваните излези се споредуваат со постигнатите системски излези, а потоа системите се прилагодуваат со прилагодување на тежините на конекција за да се намали разликата помеѓу двата колку што е можно повеќе. Алгоритмот го добива своето име затоа што тежините се ажурираат наназад, од излез кон влез.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7921f2a-9db8-42e8-9da2-1eb4196c062a",
   "metadata": {},
   "source": [
    "Градиентското спуштање \n",
    "- Е начин моделот чекор по чекор да учи како да прави сè поточни предвидувања со намалување на грешката. \n",
    "- Со секоја итерација: (Моделот прави предвидување, Се мери грешката, Се пресметува градиентот. Параметрите се прилагодуваат малку за да се намали грешката)\n",
    "- Додека функцијата не е блиску до или еднаква на нула, моделот ќе продолжи да ги прилагодува своите параметри за да даде најмала можна грешка\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a04d239-6b59-49c0-9677-b70c3d2c865d",
   "metadata": {},
   "source": [
    "Активациската функција \n",
    "- Одлучува дали невронот ќе биде активиран или не. Ова значи дека ќе одлучи дали невронскиот влез во мрежата е важен или не во процесот на предвидување користеќи едноставни математички операции.\n",
    "Улогата на активациската функција е да изведе излез од множеството на влезни вредности што се доставуваат до јазол(node) или слој(layer).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82983ec8-b67b-4aae-a7a3-c2c003514351",
   "metadata": {},
   "source": [
    "Објасни кои сличности и разлики ги имаат Seq2Seq и трансформери?\n",
    "- И двата имаат енкодер што го обработува влезот и декодер што генерира излезна секвенца\n",
    "- Најчесто се користат за задачи каде влезот и излезот се сквенци, како превод, резимирање, одговор на прашања.\n",
    "- И двата модели се обидуваат да го фатат значењето на зборовите во контекст на целата реченица.\n",
    "- Разликите се во тоа што seq2seq ги обработуваат податоците последователно, додека трансформерите паралелно, seq2seq користат LSTM и можат да користат само податоци од минатото додека трансформерите преку self-attention ги земаат и минатото и иднината во контекст.\n",
    "- Трансформерите се подобри за долги секвенци, побрзо се тренираат но бараат повеќе ресурси и сложена архитектура."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f766f-2e83-41bb-a330-59b347341819",
   "metadata": {},
   "source": [
    "Кој алагоритам на кластерирање има проблем со локален минимум. \n",
    "- K-means е чувствителен на иницијализација и затоа може да дава резултати зависни од тоа каде започнува – токму тоа е проблемот со локален минимум.\n",
    "- Се решава со користење на к-минс++ иницијализација за подобар избор на почетни центроиди. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8c4b8-5d48-4310-8c57-01f69f3a3684",
   "metadata": {},
   "source": [
    "Според Universal Approximation Theorem доволно е еден скриен слој во невронската мрежа за да се претстави апроксимација на било кој функција до одредена мера на точност. Сепак се препорачува користење на подлабоки невронски мрежи со повеќе скриени слоеви. Објасни зошто?\n",
    "- Иако еден скриен слој е теоретски доволен, подлабоки мрежи со повеќе слоеви се практични, поефикасни и полесни за тренирање, особено кај сложени реални задачи.\n",
    "- Еден слој треба многу неврони за да аппроксимира сложена функција, што може да стане непрактично. Подлабоки мрежи можат да ја поделат функцијата на помали, едноставни функции во секој слој.\n",
    "- Многу функции во реалниот свет имаат хиерархиска структура, Подлабоки мрежи можат по природен пат да учат нивоа на абстракција, што е потешко со само еден слој.\n",
    "- Подлабоки мрежи често генерализираат подобро, бидејќи секој слој учи различен аспект на функцијата, наместо еден огромен слој кој се обидува да покрие сè одеднаш."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f099a0-db2d-4bd1-9f8b-a0b81bbb9182",
   "metadata": {},
   "source": [
    "So kakvi predizvici moze da se sooci k-means algoritamot i da se dade realen primer kade takov predizvik bi napravil problem? *prviot del go imat pogore kaj sporedba so DBSCAN\n",
    "- Реален пример\n",
    "Сценарио: Кластерирање на клиенти според:\n",
    "Годишен доход: 20,000 – 150,000 USD\n",
    "Број на интернет купувања месечно: 1 – 15\n",
    "Проблеми со K-means:\n",
    "Ако не знаете колку кластери да поставите, K-means може да создаде премногу или премалку групи.\n",
    "Ако не се нормализираат податоците, доходот (поголеми бројки) ќе доминира и алгоритмот ќе ги класифицира само според доходот, игнорирајќи го бројот на купувања.\n",
    "Ако има неколку екстремно богати клиенти (outliers), K-means ќе помести центроид и ќе ги наруши кластерите.\n",
    "Резултат: Кластери кои не одразуваат вистинската структура на податоците, што ги прави неточни и неприменливи за маркетинг стратегии."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66f65d-e4e6-4ef2-977f-83f4ef74b351",
   "metadata": {},
   "source": [
    "df[\"soil_temperature\"] = df[\"sensor_packet\"].apply(lambda value: float(value.split(\"|\")[0][2:]) if not pd.isna(value) else value)\n",
    "df[\"soil_nitrogen\"] = df[\"sensor_packet\"].apply(lambda value: float(value.split(\"|\")[1][2:]) if not pd.isna(value) else value)\n",
    "df.drop(columns=[\"sensor_packet\"], inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
